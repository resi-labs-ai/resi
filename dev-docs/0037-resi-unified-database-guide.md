# RESI Unified Real Estate Database - Complete Implementation Guide

## Project Overview

This document provides a complete implementation guide for building a unified, attested real estate database that processes miner-uploaded Parquet files from S3 into a Supabase PostgreSQL database with continuous AWS-hosted processing.

## Executive Summary

Based on comprehensive analysis of the RESI subnet codebase, miners upload **incremental data (deltas)** using sophisticated offset tracking. This creates an ideal foundation for building a cost-effective, scalable unified database that:

- âœ… **Processes real-time miner submissions** from S3 Parquet files
- âœ… **Builds consensus data** with confidence scoring
- âœ… **Prevents abuse** through trust-based rate limiting
- âœ… **Stays cost-effective** using Supabase Pro ($25/month)
- âœ… **Scales automatically** with AWS Lambda + SQS architecture

## How Miners Currently Upload Data

### ðŸ” **Key Finding: Miners Upload DELTAS, Not Full Datasets**

**Upload Pattern:**
- **Frequency**: Every 2 hours (mainnet) / 5 minutes (testnet)
- **Method**: Incremental uploads using offset-based state tracking
- **Structure**: `hotkey={miner_hotkey}/job_id={job_id}/data_{timestamp}_{record_count}.parquet`
- **State Management**: Each miner tracks `last_offset` per job_id to avoid re-uploading existing data

**Example Upload Cycle:**
```
Cycle 1: zillow_zip_77494: Records 0-1000 (1000 new records)
Cycle 2: zillow_zip_77494: Records 1000-1050 (50 NEW records only)  
Cycle 3: zillow_zip_77494: No new data â†’ Skip upload
```

**State Tracking System:**
```json
{
  "zillow_zip_77494": {
    "last_offset": 1050,
    "total_records_processed": 1050,
    "last_processed_time": "2025-09-17T15:30:00",
    "processing_completed": false
  }
}
```

### ðŸ“Š **Data Schema Analysis**

**Core Fields in Parquet Files:**
- `zpid` (Zillow Property ID) - Primary unique identifier
- `uri` - Unique URL identifier  
- `datetime` - Scrape timestamp
- `price`, `address`, `bedrooms`, `bathrooms`, `living_area`
- `home_type`, `home_status`, `days_on_zillow`
- `latitude`, `longitude`, `zip_code`, `city`, `state`

**Attestation Metadata (Embedded in S3 Structure):**
- **Miner Identity**: `hotkey={miner_hotkey}` in S3 path
- **Submission Time**: Timestamp in filename `data_20250917_153000_330.parquet`
- **Data Volume**: Record count in filename
- **Job Context**: `job_id={job_id}` indicates scraping parameters

## AWS Hosting Architecture for Continuous Processing

### ðŸ—ï¸ **Complete AWS Architecture**

```
Miner S3 Bucket (Source Data)
    â†“ (S3 Event Notification)
SQS Queue (resi-parquet-processing)
    â†“ (Lambda Trigger - Batch Processing)
Lambda Function (parquet-processor)
    â†“ (Database Updates via Prisma)
Supabase PostgreSQL Database
    â†“ (API Access)
API Gateway + Lambda (Public API Endpoints)
    â†“ (Monitoring)
CloudWatch (Logs, Metrics, Alarms)
```

### ðŸš€ **Key AWS Components**

**1. S3 Event-Driven Processing:**
- **Trigger**: New .parquet files uploaded by miners
- **Filter**: Only process files matching pattern `data_*.parquet`
- **Destination**: SQS queue for reliable processing

**2. SQS Queue Configuration:**
- **Visibility Timeout**: 300 seconds (5 min for Lambda processing)
- **Message Retention**: 14 days
- **Dead Letter Queue**: For failed processing attempts
- **Batch Size**: 10 messages per Lambda invocation

**3. Lambda Function Specs:**
- **Runtime**: Node.js 18.x
- **Memory**: 1024 MB (1GB RAM)
- **Timeout**: 300 seconds (5 minutes)
- **Environment**: DATABASE_URL, AWS region configs
- **Triggers**: SQS queue messages

## Database Schema with Prisma

### ðŸ—„ï¸ **Complete Prisma Schema**

**File: `prisma/schema.prisma`**
```prisma
generator client {
  provider = "prisma-client-js"
}

datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

model Listing {
  zpid                String   @id @db.VarChar(50)
  uri                 String   @unique @db.VarChar(500)
  
  // Property data
  price               BigInt?
  address             String?
  bedrooms            Int?
  bathrooms           Decimal? @db.Decimal(3,1)
  living_area         Int?
  home_type           String?  @db.VarChar(50)
  home_status         String?  @db.VarChar(50)
  days_on_zillow      Int?
  zip_code            String?  @db.VarChar(10)
  city                String?  @db.VarChar(100)
  state               String?  @db.VarChar(10)
  latitude            Decimal? @db.Decimal(10,8)
  longitude           Decimal? @db.Decimal(11,8)
  
  // Consensus metadata
  first_seen_at       DateTime @default(now()) @db.Timestamptz
  last_updated_at     DateTime @updatedAt @db.Timestamptz
  consensus_score     Decimal? @db.Decimal(3,2)
  total_attestations  Int      @default(1)
  unique_miners_count Int      @default(1)
  
  // Relations
  miner_attestations  MinerAttestation[]
  
  @@index([zip_code])
  @@index([price])
  @@index([latitude, longitude])
  @@index([last_updated_at])
  @@index([home_status])
}

model MinerAttestation {
  id                    BigInt   @id @default(autoincrement())
  
  // Miner identification
  miner_hotkey          String   @db.VarChar(100)
  miner_uid             Int?
  
  // Submission details
  zpid                  String   @db.VarChar(50)
  submission_timestamp  DateTime @db.Timestamptz
  job_id                String?  @db.VarChar(100)
  s3_file_path          String?  @db.VarChar(500)
  record_count          Int?
  
  // Data integrity
  data_hash             String?  @db.VarChar(64)
  
  // Validation status
  is_validated          Boolean  @default(false)
  validation_timestamp  DateTime? @db.Timestamptz
  validation_score      Decimal? @db.Decimal(3,2)
  
  // Relations
  listing               Listing  @relation(fields: [zpid], references: [zpid])
  miner_trust           MinerTrust @relation(fields: [miner_hotkey], references: [miner_hotkey])
  
  @@index([miner_hotkey])
  @@index([submission_timestamp])
  @@index([zpid])
  @@index([data_hash])
}

model MinerTrust {
  miner_hotkey          String   @id @db.VarChar(100)
  trust_score           Decimal  @default(0.5) @db.Decimal(3,2)
  total_submissions     BigInt   @default(0)
  validated_submissions BigInt   @default(0)
  last_submission       DateTime? @db.Timestamptz
  
  // Rate limiting
  submissions_today     Int      @default(0)
  last_submission_date  DateTime? @db.Date
  
  // Quality metrics
  duplicate_rate        Decimal  @default(0.0) @db.Decimal(3,2)
  accuracy_score        Decimal  @default(0.5) @db.Decimal(3,2)
  
  // Relations
  attestations          MinerAttestation[]
  
  @@index([trust_score])
  @@index([last_submission])
}
```

## Complete Implementation Guide

### ðŸ“‹ **Phase 1: Project Setup (Day 1-2)**

**1. Repository Creation:**
```bash
mkdir resi-unified-database
cd resi-unified-database
git init

# Create directory structure
mkdir -p src/{database/schema,ingestion/lambda,api,trust,consensus}
mkdir -p infrastructure/{aws,supabase,monitoring}
mkdir -p docs tests scripts

npm init -y
```

**2. Dependencies Installation:**
```bash
# Core database and AWS
npm install @prisma/client prisma aws-sdk
npm install parquetjs crypto

# API framework
npm install express cors helmet compression
npm install express-rate-limit express-validator

# Utilities
npm install dotenv uuid moment

# Development tools
npm install -D typescript @types/node ts-node
npm install -D jest @types/jest supertest
npm install -D nodemon concurrently
```

**3. Environment Setup (`.env`):**
```env
# Supabase Database (from your Supabase project settings)
DATABASE_URL="postgresql://postgres:[PASSWORD]@db.[PROJECT-REF].supabase.co:5432/postgres"

# AWS Configuration  
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your-access-key-id
AWS_SECRET_ACCESS_KEY=your-secret-access-key

# S3 Configuration (where miners upload parquet files)
MINER_S3_BUCKET=your-miner-s3-bucket-name
MINER_S3_BUCKET_ARN=arn:aws:s3:::your-miner-s3-bucket-name

# Application Settings
NODE_ENV=development
PORT=3000
API_RATE_LIMIT=100  # requests per hour per IP
```

### ðŸ—„ï¸ **Phase 2: Database Setup with Supabase (Day 3-4)**

**1. Supabase Project Creation:**
- Visit [supabase.com/dashboard](https://supabase.com/dashboard)
- Click "New Project"
- Choose **Pro Plan ($25/month)** for production features
- Select region closest to your users
- Save the DATABASE_URL connection string

**2. Prisma Schema Setup:**

Create the Prisma schema file above, then:

```bash
# Generate Prisma client
npx prisma generate

# Create and run initial migration
npx prisma migrate dev --name initial_schema

# Verify database connection
npx prisma studio  # Opens web interface to view data
```

**3. Database Connection Test:**
```javascript
// scripts/test-db-connection.js
const { PrismaClient } = require('@prisma/client');
const prisma = new PrismaClient();

async function testConnection() {
    try {
        // Test basic connection
        const result = await prisma.$queryRaw`SELECT version()`;
        console.log('âœ… Database connected successfully');
        console.log('PostgreSQL version:', result[0].version);
        
        // Test creating a sample record
        const testMiner = await prisma.minerTrust.upsert({
            where: { miner_hotkey: 'test_miner_123' },
            update: { trust_score: 0.6 },
            create: { 
                miner_hotkey: 'test_miner_123',
                trust_score: 0.5,
                total_submissions: 0
            }
        });
        console.log('âœ… Test record created:', testMiner);
        
    } catch (error) {
        console.error('âŒ Database connection failed:', error);
        process.exit(1);
    } finally {
        await prisma.$disconnect();
    }
}

testConnection();
```

### â˜ï¸ **Phase 3: AWS Infrastructure (Day 5-7)**

**1. Terraform Setup:**

Create `infrastructure/aws/main.tf`:
```hcl
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
}

# Variables
variable "aws_region" {
  description = "AWS region"
  default     = "us-east-1"
}

variable "database_url" {
  description = "Supabase database URL"
  type        = string
  sensitive   = true
}

variable "miner_s3_bucket" {
  description = "S3 bucket where miners upload parquet files"
  type        = string
}

# SQS Queue for processing parquet files
resource "aws_sqs_queue" "parquet_processing_queue" {
  name = "resi-parquet-processing"
  
  # Lambda processing timeout + buffer
  visibility_timeout_seconds = 360  
  
  # Retain messages for 14 days
  message_retention_seconds = 1209600
  
  # Dead letter queue for failed messages
  redrive_policy = jsonencode({
    deadLetterTargetArn = aws_sqs_queue.parquet_processing_dlq.arn
    maxReceiveCount     = 3
  })

  tags = {
    Project = "resi-unified-database"
  }
}

# Dead Letter Queue
resource "aws_sqs_queue" "parquet_processing_dlq" {
  name = "resi-parquet-processing-dlq"
  message_retention_seconds = 1209600
  
  tags = {
    Project = "resi-unified-database"
  }
}

# SQS Queue Policy (allow S3 to send messages)
resource "aws_sqs_queue_policy" "parquet_processing_policy" {
  queue_url = aws_sqs_queue.parquet_processing_queue.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Service = "s3.amazonaws.com"
        }
        Action   = "sqs:SendMessage"
        Resource = aws_sqs_queue.parquet_processing_queue.arn
        Condition = {
          ArnEquals = {
            "aws:SourceArn" = "arn:aws:s3:::${var.miner_s3_bucket}"
          }
        }
      }
    ]
  })
}

# Lambda execution role
resource "aws_iam_role" "lambda_execution_role" {
  name = "resi-lambda-execution-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}

# Lambda execution policy
resource "aws_iam_role_policy" "lambda_execution_policy" {
  name = "resi-lambda-execution-policy"
  role = aws_iam_role.lambda_execution_role.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "logs:CreateLogGroup",
          "logs:CreateLogStream",
          "logs:PutLogEvents"
        ]
        Resource = "arn:aws:logs:*:*:*"
      },
      {
        Effect = "Allow"
        Action = [
          "sqs:ReceiveMessage",
          "sqs:DeleteMessage",
          "sqs:GetQueueAttributes"
        ]
        Resource = aws_sqs_queue.parquet_processing_queue.arn
      },
      {
        Effect = "Allow"
        Action = [
          "s3:GetObject"
        ]
        Resource = "arn:aws:s3:::${var.miner_s3_bucket}/*"
      }
    ]
  })
}

# Lambda function for processing parquet files
resource "aws_lambda_function" "parquet_processor" {
  filename         = "parquet-processor.zip"
  function_name    = "resi-parquet-processor"
  role            = aws_iam_role.lambda_execution_role.arn
  handler         = "index.handler"
  source_code_hash = filebase64sha256("parquet-processor.zip")
  
  runtime     = "nodejs18.x"
  timeout     = 300  # 5 minutes
  memory_size = 1024 # 1GB RAM

  environment {
    variables = {
      DATABASE_URL = var.database_url
      NODE_ENV     = "production"
    }
  }

  tags = {
    Project = "resi-unified-database"
  }
}

# SQS trigger for Lambda
resource "aws_lambda_event_source_mapping" "sqs_lambda_trigger" {
  event_source_arn = aws_sqs_queue.parquet_processing_queue.arn
  function_name    = aws_lambda_function.parquet_processor.arn
  batch_size       = 5  # Process 5 messages at once
  maximum_batching_window_in_seconds = 10
}

# S3 bucket notification to SQS
resource "aws_s3_bucket_notification" "miner_data_notification" {
  bucket = var.miner_s3_bucket

  queue {
    queue_arn = aws_sqs_queue.parquet_processing_queue.arn
    events    = ["s3:ObjectCreated:*"]
    
    # Only trigger on parquet files in the data/ prefix
    filter_prefix = "data/"
    filter_suffix = ".parquet"
  }

  depends_on = [aws_sqs_queue_policy.parquet_processing_policy]
}

# Outputs
output "sqs_queue_url" {
  value = aws_sqs_queue.parquet_processing_queue.id
}

output "lambda_function_name" {
  value = aws_lambda_function.parquet_processor.function_name
}
```

**2. Deploy Infrastructure:**
```bash
cd infrastructure/aws

# Initialize Terraform
terraform init

# Create deployment package for Lambda (we'll build this in next phase)
# For now, create empty zip
echo '{}' > package.json
zip parquet-processor.zip package.json

# Plan deployment
terraform plan \
  -var="database_url=$DATABASE_URL" \
  -var="miner_s3_bucket=$MINER_S3_BUCKET"

# Apply infrastructure
terraform apply
```

### ðŸ”„ **Phase 4: Real-time Processing (Day 8-10)**

**1. Lambda Function Implementation:**

Create `src/ingestion/lambda/index.js`:
```javascript
const { PrismaClient } = require('@prisma/client');
const AWS = require('aws-sdk');
const crypto = require('crypto');

const prisma = new PrismaClient();
const s3 = new AWS.S3();

exports.handler = async (event) => {
    console.log(`Processing ${event.Records.length} messages`);
    
    const results = {
        processed: 0,
        errors: 0,
        skipped: 0
    };
    
    for (const record of event.Records) {
        try {
            const result = await processMessage(record);
            results[result]++;
        } catch (error) {
            console.error('Message processing failed:', error);
            results.errors++;
            // Don't throw - let other messages process
        }
    }
    
    console.log('Processing results:', results);
    return { statusCode: 200, body: JSON.stringify(results) };
};

async function processMessage(sqsRecord) {
    // Parse S3 event from SQS message
    const s3Event = JSON.parse(sqsRecord.body);
    const s3Record = s3Event.Records[0];
    
    const bucket = s3Record.s3.bucket.name;
    const key = s3Record.s3.object.key;
    
    console.log(`Processing: s3://${bucket}/${key}`);
    
    // Extract file metadata from S3 path
    const fileInfo = extractFileMetadata(key);
    if (!fileInfo) {
        console.warn(`Invalid file path format: ${key}`);
        return 'skipped';
    }
    
    // Check miner rate limits
    const rateLimitOk = await checkMinerRateLimit(fileInfo.minerHotkey);
    if (!rateLimitOk) {
        console.warn(`Rate limit exceeded for miner: ${fileInfo.minerHotkey}`);
        return 'skipped';
    }
    
    // Download and process parquet file
    await processParquetFile(bucket, key, fileInfo);
    
    return 'processed';
}

function extractFileMetadata(s3Key) {
    // Parse: data/hotkey=5C7VLTuy.../job_id=zillow_zip_77494/data_20250917_153000_330.parquet
    const match = s3Key.match(/hotkey=([^/]+)\/job_id=([^/]+)\/data_(\d{8}_\d{6})_(\d+)\.parquet$/);
    if (!match) return null;
    
    return {
        minerHotkey: match[1],
        jobId: match[2], 
        timestamp: new Date(match[3].replace(/(\d{4})(\d{2})(\d{2})_(\d{2})(\d{2})(\d{2})/, '$1-$2-$3T$4:$5:$6Z')),
        recordCount: parseInt(match[4])
    };
}

async function checkMinerRateLimit(minerHotkey) {
    const today = new Date().toISOString().split('T')[0];
    
    const minerTrust = await prisma.minerTrust.findUnique({
        where: { miner_hotkey: minerHotkey }
    });
    
    if (!minerTrust) {
        // First time seeing this miner - create trust record
        await prisma.minerTrust.create({
            data: {
                miner_hotkey: minerHotkey,
                trust_score: 0.5,
                submissions_today: 0,
                last_submission_date: new Date(today)
            }
        });
        return true;
    }
    
    // Reset daily counter if new day
    if (minerTrust.last_submission_date?.toISOString().split('T')[0] !== today) {
        await prisma.minerTrust.update({
            where: { miner_hotkey: minerHotkey },
            data: {
                submissions_today: 0,
                last_submission_date: new Date(today)
            }
        });
        return true;
    }
    
    // Calculate rate limit based on trust score
    const trustScore = parseFloat(minerTrust.trust_score);
    const maxDaily = trustScore >= 0.8 ? 10000 : 
                    trustScore >= 0.6 ? 5000 : 
                    trustScore >= 0.4 ? 1000 : 100;
    
    return minerTrust.submissions_today < maxDaily;
}

async function processParquetFile(bucket, key, fileInfo) {
    // Download parquet file from S3
    const s3Object = await s3.getObject({ Bucket: bucket, Key: key }).promise();
    
    // Parse parquet data (simplified - in production use proper parquet library)
    const listings = await parseParquetBuffer(s3Object.Body);
    
    console.log(`Processing ${listings.length} listings from ${fileInfo.minerHotkey}`);
    
    // Process each listing with database transaction
    await prisma.$transaction(async (tx) => {
        for (const listing of listings) {
            await processListing(tx, listing, fileInfo, `s3://${bucket}/${key}`);
        }
        
        // Update miner statistics
        await updateMinerStats(tx, fileInfo.minerHotkey, listings.length);
    });
}

async function parseParquetBuffer(buffer) {
    // Simplified parquet parsing - replace with proper parquet library
    // For now, assume JSON format for testing
    try {
        const jsonData = JSON.parse(buffer.toString());
        return Array.isArray(jsonData) ? jsonData : [jsonData];
    } catch (error) {
        console.error('Parquet parsing failed:', error);
        return [];
    }
}

async function processListing(tx, listing, fileInfo, s3Path) {
    const zpid = listing.zpid;
    
    // Calculate data hash for deduplication
    const dataHash = crypto.createHash('sha256')
        .update(JSON.stringify({
            zpid: listing.zpid,
            price: listing.price,
            address: listing.address,
            bedrooms: listing.bedrooms,
            bathrooms: listing.bathrooms
        }))
        .digest('hex');
    
    // Check if we've already processed this exact data
    const existingAttestation = await tx.minerAttestation.findFirst({
        where: {
            zpid,
            miner_hotkey: fileInfo.minerHotkey,
            data_hash: dataHash
        }
    });
    
    if (existingAttestation) {
        console.log(`Duplicate data for zpid ${zpid} from ${fileInfo.minerHotkey}`);
        return;
    }
    
    // Upsert listing with consensus logic
    await tx.listing.upsert({
        where: { zpid },
        update: {
            // Update with new data if price changed significantly
            price: listing.price ? BigInt(listing.price) : null,
            last_updated_at: new Date(),
            total_attestations: { increment: 1 }
        },
        create: {
            zpid,
            uri: listing.uri,
            price: listing.price ? BigInt(listing.price) : null,
            address: listing.address,
            bedrooms: listing.bedrooms,
            bathrooms: listing.bathrooms,
            living_area: listing.living_area,
            home_type: listing.home_type,
            home_status: listing.home_status,
            days_on_zillow: listing.days_on_zillow,
            zip_code: listing.zip_code,
            city: listing.city,
            state: listing.state,
            latitude: listing.latitude,
            longitude: listing.longitude,
            consensus_score: 1.0,
            unique_miners_count: 1
        }
    });
    
    // Record miner attestation
    await tx.minerAttestation.create({
        data: {
            miner_hotkey: fileInfo.minerHotkey,
            zpid,
            submission_timestamp: fileInfo.timestamp,
            job_id: fileInfo.jobId,
            s3_file_path: s3Path,
            record_count: fileInfo.recordCount,
            data_hash: dataHash
        }
    });
}

async function updateMinerStats(tx, minerHotkey, listingCount) {
    await tx.minerTrust.update({
        where: { miner_hotkey: minerHotkey },
        data: {
            total_submissions: { increment: BigInt(listingCount) },
            submissions_today: { increment: listingCount },
            last_submission: new Date()
        }
    });
}
```

**2. Package and Deploy Lambda:**
```bash
cd src/ingestion/lambda

# Install production dependencies
npm install --production @prisma/client aws-sdk crypto

# Generate Prisma client for Lambda
npx prisma generate

# Create deployment package
zip -r ../../../infrastructure/aws/parquet-processor.zip .

# Redeploy with new code
cd ../../../infrastructure/aws
terraform apply -var="database_url=$DATABASE_URL" -var="miner_s3_bucket=$MINER_S3_BUCKET"
```

### ðŸŒ **Phase 5: API Development (Day 11-12)**

**1. Create API Server (`src/api/server.js`):**
```javascript
const express = require('express');
const cors = require('cors');
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');
const { PrismaClient } = require('@prisma/client');

const app = express();
const prisma = new PrismaClient();

// Middleware
app.use(helmet());
app.use(cors());
app.use(express.json());

// Rate limiting
const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100 // limit each IP to 100 requests per windowMs
});
app.use('/api/', limiter);

// Health check
app.get('/health', (req, res) => {
  res.json({ status: 'healthy', timestamp: new Date().toISOString() });
});

// Get listings by zip code
app.get('/api/listings/:zipCode', async (req, res) => {
  try {
    const { zipCode } = req.params;
    const { limit = 100, offset = 0, minPrice, maxPrice } = req.query;
    
    const where = { zip_code: zipCode };
    if (minPrice || maxPrice) {
      where.price = {};
      if (minPrice) where.price.gte = BigInt(minPrice);
      if (maxPrice) where.price.lte = BigInt(maxPrice);
    }
    
    const listings = await prisma.listing.findMany({
      where,
      include: {
        miner_attestations: {
          select: {
            miner_hotkey: true,
            submission_timestamp: true,
            validation_score: true
          },
          take: 5, // Limit attestations shown
          orderBy: { submission_timestamp: 'desc' }
        }
      },
      take: parseInt(limit),
      skip: parseInt(offset),
      orderBy: { last_updated_at: 'desc' }
    });
    
    // Convert BigInt to string for JSON serialization
    const serializedListings = listings.map(listing => ({
      ...listing,
      price: listing.price?.toString()
    }));
    
    res.json({
      listings: serializedListings,
      pagination: {
        limit: parseInt(limit),
        offset: parseInt(offset),
        total: await prisma.listing.count({ where })
      }
    });
    
  } catch (error) {
    console.error('Error fetching listings:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// Get specific listing by zpid
app.get('/api/listing/:zpid', async (req, res) => {
  try {
    const { zpid } = req.params;
    
    const listing = await prisma.listing.findUnique({
      where: { zpid },
      include: {
        miner_attestations: {
          include: {
            miner_trust: {
              select: {
                trust_score: true,
                total_submissions: true
              }
            }
          },
          orderBy: { submission_timestamp: 'desc' }
        }
      }
    });
    
    if (!listing) {
      return res.status(404).json({ error: 'Listing not found' });
    }
    
    // Convert BigInt and add consensus info
    const response = {
      ...listing,
      price: listing.price?.toString(),
      total_submissions: listing.total_submissions?.toString(),
      consensus_info: {
        total_attestations: listing.total_attestations,
        unique_miners: listing.unique_miners_count,
        consensus_score: listing.consensus_score,
        confidence_level: parseFloat(listing.consensus_score) >= 0.8 ? 'high' : 
                         parseFloat(listing.consensus_score) >= 0.6 ? 'medium' : 'low'
      }
    };
    
    res.json(response);
    
  } catch (error) {
    console.error('Error fetching listing:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// Get miner statistics
app.get('/api/miners/:hotkey/stats', async (req, res) => {
  try {
    const { hotkey } = req.params;
    
    const minerStats = await prisma.minerTrust.findUnique({
      where: { miner_hotkey: hotkey },
      include: {
        attestations: {
          select: {
            submission_timestamp: true,
            is_validated: true,
            zpid: true
          },
          take: 100, // Last 100 submissions
          orderBy: { submission_timestamp: 'desc' }
        }
      }
    });
    
    if (!minerStats) {
      return res.status(404).json({ error: 'Miner not found' });
    }
    
    // Calculate additional metrics
    const recentSubmissions = minerStats.attestations.filter(
      a => new Date() - new Date(a.submission_timestamp) < 24 * 60 * 60 * 1000 // Last 24 hours
    ).length;
    
    const response = {
      miner_hotkey: minerStats.miner_hotkey,
      trust_score: minerStats.trust_score,
      total_submissions: minerStats.total_submissions?.toString(),
      validated_submissions: minerStats.validated_submissions?.toString(),
      submissions_today: minerStats.submissions_today,
      recent_submissions_24h: recentSubmissions,
      last_submission: minerStats.last_submission,
      accuracy_score: minerStats.accuracy_score,
      duplicate_rate: minerStats.duplicate_rate,
      performance_tier: parseFloat(minerStats.trust_score) >= 0.8 ? 'premium' :
                       parseFloat(minerStats.trust_score) >= 0.6 ? 'standard' : 'basic'
    };
    
    res.json(response);
    
  } catch (error) {
    console.error('Error fetching miner stats:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

// Get database statistics
app.get('/api/stats', async (req, res) => {
  try {
    const [
      totalListings,
      totalMiners,
      recentListings,
      avgConsensusScore
    ] = await Promise.all([
      prisma.listing.count(),
      prisma.minerTrust.count(),
      prisma.listing.count({
        where: {
          last_updated_at: {
            gte: new Date(Date.now() - 24 * 60 * 60 * 1000) // Last 24 hours
          }
        }
      }),
      prisma.listing.aggregate({
        _avg: {
          consensus_score: true
        }
      })
    ]);
    
    res.json({
      total_listings: totalListings,
      total_miners: totalMiners,
      recent_listings_24h: recentListings,
      average_consensus_score: avgConsensusScore._avg.consensus_score,
      last_updated: new Date().toISOString()
    });
    
  } catch (error) {
    console.error('Error fetching stats:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`ðŸš€ RESI Unified Database API server running on port ${PORT}`);
  console.log(`ðŸ“Š Health check: http://localhost:${PORT}/health`);
  console.log(`ðŸ“ˆ Stats endpoint: http://localhost:${PORT}/api/stats`);
});
```

**2. Test API Locally:**
```bash
cd src/api
npm install express cors helmet express-rate-limit @prisma/client
node server.js

# Test endpoints
curl http://localhost:3000/health
curl http://localhost:3000/api/stats
```

### ðŸ“Š **Phase 6: Monitoring & Production (Day 13-14)**

**1. CloudWatch Monitoring (`infrastructure/aws/monitoring.tf`):**
```hcl
# CloudWatch Log Group for Lambda
resource "aws_cloudwatch_log_group" "lambda_logs" {
  name              = "/aws/lambda/resi-parquet-processor"
  retention_in_days = 14
}

# CloudWatch Alarms
resource "aws_cloudwatch_metric_alarm" "lambda_errors" {
  alarm_name          = "resi-lambda-errors"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "Errors"
  namespace           = "AWS/Lambda"
  period              = "300"
  statistic           = "Sum"
  threshold           = "5"
  alarm_description   = "This metric monitors lambda errors"

  dimensions = {
    FunctionName = aws_lambda_function.parquet_processor.function_name
  }
}

resource "aws_cloudwatch_metric_alarm" "sqs_queue_depth" {
  alarm_name          = "resi-sqs-queue-depth"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "ApproximateNumberOfVisibleMessages"
  namespace           = "AWS/SQS"
  period              = "300"
  statistic           = "Average"
  threshold           = "100"
  alarm_description   = "SQS queue has too many pending messages"

  dimensions = {
    QueueName = aws_sqs_queue.parquet_processing_queue.name
  }
}
```

**2. Database Monitoring Queries:**
```sql
-- Monitor database size
SELECT 
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size
FROM pg_tables 
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Data quality metrics
SELECT 
  COUNT(*) as total_listings,
  AVG(consensus_score) as avg_consensus,
  COUNT(CASE WHEN consensus_score > 0.8 THEN 1 END) as high_confidence,
  COUNT(CASE WHEN unique_miners_count > 3 THEN 1 END) as multi_attested
FROM listings
WHERE last_updated_at > NOW() - INTERVAL '24 hours';

-- Miner performance leaderboard
SELECT 
  miner_hotkey,
  trust_score,
  total_submissions,
  validated_submissions,
  (validated_submissions::float / NULLIF(total_submissions, 0)) as validation_rate
FROM miner_trust
ORDER BY trust_score DESC
LIMIT 20;
```

## Final Implementation Plan Summary

### ðŸŽ¯ **Complete 2-Week Implementation Timeline**

**Week 1: Foundation & Database**
- **Days 1-2**: Project setup, dependencies, repository structure
- **Days 3-4**: Supabase setup, Prisma schema, database testing  
- **Days 5-7**: AWS infrastructure deployment, Lambda functions

**Week 2: Processing & API**
- **Days 8-10**: Complete parquet processing pipeline, testing with real data
- **Days 11-12**: Build REST API endpoints, authentication
- **Days 13-14**: Monitoring, documentation, production deployment

### ðŸ’° **Cost Breakdown & Revenue Model**

**Monthly Operating Costs:**
- **Supabase Pro**: $25/month (8GB database, 250GB bandwidth)
- **AWS Lambda**: $10-20/month (parquet processing)
- **AWS SQS**: $2-5/month (message queuing)
- **AWS CloudWatch**: $5-10/month (monitoring)
- **Total**: ~$42-60/month

**Revenue Streams:**
- **API Access**: $50-200/month per developer
- **Enterprise Plans**: $500-2000/month for high-volume users
- **Real Estate Data Feeds**: $1000+/month for MLS integrations
- **Break-even**: 2-3 API customers

### âœ… **Success Metrics**

**Technical KPIs:**
- **Processing Speed**: <5 minutes from S3 upload to database
- **Data Quality**: >95% consensus accuracy across miners
- **API Performance**: <200ms average response time
- **Uptime**: 99.9% availability

**Business KPIs:**
- **Customer Growth**: 10+ API customers in first 6 months
- **Data Volume**: 5M+ listings processed
- **Revenue**: $1000+/month by month 6
- **Cost Efficiency**: <30% of revenue spent on infrastructure

### ðŸš€ **Deployment Commands**

**Quick Start:**
```bash
# Clone and setup
git clone <your-repo>
cd resi-unified-database
npm install

# Setup environment
cp .env.example .env
# Edit .env with your Supabase and AWS credentials

# Database setup
npx prisma generate
npx prisma migrate dev --name initial_schema

# Deploy AWS infrastructure
cd infrastructure/aws
terraform init
terraform apply

# Deploy Lambda function
cd ../../src/ingestion/lambda
npm install --production
zip -r ../../../infrastructure/aws/parquet-processor.zip .
cd ../../../infrastructure/aws
terraform apply

# Start API server
cd ../../src/api
npm install
node server.js
```

This comprehensive implementation guide provides everything needed to build a production-ready, scalable RESI unified database system. The architecture leverages modern cloud technologies while maintaining cost-effectiveness and high data quality through consensus mechanisms.
