# The MIT L# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

import copy
import json
import sys
import os
# Add the project root to Python path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# The MIT License (MIT)
# Copyright © 2025 Resi Labs

# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the “Software”), to deal in the Software without restriction, including without limitation
# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,
# and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in all copies or substantial portions of
# the Software.

# THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO
# THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
# DEALINGS IN THE SOFTWARE.

import copy
import json
import sys
import torch
import numpy as np
import asyncio
import threading
import time
import os
import wandb
import subprocess
import traceback
from common.metagraph_syncer import MetagraphSyncer
from neurons.config import NeuronType, check_config, create_config
from dynamic_desirability.desirability_retrieval import sync_run_retrieval
from neurons import __spec_version__ as spec_version
from vali_utils.validator_s3_access import ValidatorS3Access
from rewards.data_value_calculator import DataValueCalculator
from rich.table import Table
from rich.console import Console
import warnings
import requests
from dotenv import load_dotenv
import bittensor as bt
from typing import Tuple
from common.organic_protocol import OrganicRequest
from common import constants
from common import utils
from vali_utils.miner_evaluator import MinerEvaluator
from vali_utils.load_balancer.validator_registry import ValidatorRegistry
from vali_utils.organic_query_processor import OrganicQueryProcessor

from vali_utils import metrics

# Import zipcode validation components
from common.resi_api_client import create_api_client
from rewards.zipcode_competitive_scorer import ZipcodeCompetitiveScorer
from vali_utils.multi_tier_validator import MultiTierValidator
from vali_utils.deterministic_consensus import DeterministicConsensus

load_dotenv()
# Temporary solution to getting rid of annoying bittensor trace logs
original_trace = bt.logging.trace


def filtered_trace(message, *args, **kwargs):
    if "Unexpected header key encountered" not in message:
        original_trace(message, *args, **kwargs)


bt.logging.trace = filtered_trace 

# Filter out the specific deprecation warning from datetime.utcnow()
warnings.filterwarnings(
    "ignore",
    category=DeprecationWarning,
    message="datetime.datetime.utcnow() is deprecated"
)
# import datetime after the warning filter
import datetime as dt
from datetime import datetime, timezone

bt.logging.set_trace(True)  # TODO remove it in future


class Validator:
    def __init__(
        self,
        metagraph_syncer: MetagraphSyncer,
        evaluator: MinerEvaluator,
        uid: int,
        config=None,
        subtensor: bt.subtensor = None,
    ):
        """

        Args:
            metagraph_syncer (MetagraphSyncer): The syncer must already be initialized, with the initial metagraphs synced.
            evaluator (MinerEvaluator): The evaluator to evaluate miners.
            uid (int): The uid of the validator.
            config (_type_, optional): _description_. Defaults to None.
            subtensor (bt.subtensor, optional): _description_. Defaults to None.
        """
        self.metagraph_syncer = metagraph_syncer
        self.evaluator = evaluator
        self.uid = uid

        self.config = copy.deepcopy(config or create_config(NeuronType.VALIDATOR))
        check_config(self.config)

        # Auto-configure S3 auth URL based on subnet
        if self.config.netuid == 428:  # Testnet
            if self.config.s3_auth_url == "https://s3-auth-api.resilabs.ai":  # Default mainnet URL
                self.config.s3_auth_url = "https://s3-auth-api-testnet.resilabs.ai"
                bt.logging.info(f"Auto-configured testnet S3 auth URL: {self.config.s3_auth_url}")
        else:  # Mainnet or other subnets
            if not hasattr(self.config, 's3_auth_url') or not self.config.s3_auth_url:
                self.config.s3_auth_url = "https://s3-auth-api.resilabs.ai"
                bt.logging.info(f"Auto-configured mainnet S3 auth URL: {self.config.s3_auth_url}")

        bt.logging.info(self.config)

        # The wallet holds the cryptographic key pairs for the miner.
        self.wallet = bt.wallet(config=self.config)
        bt.logging.info(f"Wallet: {self.wallet}.")

        # The subtensor is our connection to the Bittensor blockchain.
        self.subtensor = subtensor or bt.subtensor(config=self.config)
        bt.logging.info(f"Subtensor: {self.subtensor}.")

        # The metagraph holds the state of the network, letting us know about other validators and miners.
        self.metagraph = self.metagraph_syncer.get_metagraph(self.config.netuid)
        self.metagraph_syncer.register_listener(
            self._on_metagraph_updated, netuids=[self.config.netuid]
        )
        bt.logging.info(f"Metagraph: {self.metagraph}.")
        
        self.validator_registry = ValidatorRegistry(metagraph=self.metagraph, organic_whitelist=self.config.organic_whitelist)
        self.organic_processor = None

        # Create asyncio event loop to manage async tasks.
        self.loop = asyncio.get_event_loop()
        self.axon = None
        self.api = None
        self.step = 0
        self.wandb_run_start = None
        self.wandb_run = None

        # Instantiate runners
        self.should_exit: bool = False
        self.is_running: bool = False
        self.thread: threading.Thread = None
        self.lock = threading.RLock()
        self.last_eval_time = dt.datetime.utcnow()
        self.last_weights_set_time = dt.datetime.utcnow()
        self.last_weights_set_block = 0  # Track the block when weights were last set
        self.is_setup = False

        # Add counter for evaluation cycles since startup
        self.evaluation_cycles_since_startup = 0
        
        # Initialize zipcode validation components (enabled by default)
        self.zipcode_validation_enabled = not getattr(self.config, 'disable_zipcode_mining', False)
        self.api_client = None
        self.zipcode_scorer = None
        self.multi_tier_validator = None
        self.consensus_manager = None
        self.current_epoch_data = None
        
        if self.zipcode_validation_enabled:
            try:
                # Validate proxy configuration for production
                self._validate_proxy_configuration()
                
                self.api_client = create_api_client(self.config, self.wallet)
                self.zipcode_scorer = ZipcodeCompetitiveScorer()
                self.multi_tier_validator = MultiTierValidator()
                self.consensus_manager = DeterministicConsensus()
                
                # Configure proxy for scraper if provided
                if hasattr(self.config, 'proxy_url') and self.config.proxy_url:
                    self._configure_scraper_proxy()
                
                bt.logging.success("Zipcode validation system initialized")
                
                # Test API connectivity
                health = self.api_client.check_health()
                if health.get('status') == 'ok':
                    bt.logging.success(f"API server connected: {health.get('service', 'Unknown')}")
                else:
                    bt.logging.warning("API server health check failed, but continuing...")
                    
            except Exception as e:
                bt.logging.error(f"Failed to initialize zipcode validation system: {e}")
                bt.logging.warning("Continuing without zipcode validation functionality")
                self.zipcode_validation_enabled = False

    def _validate_proxy_configuration(self):
        """
        Validate proxy configuration for production validators
        
        Validators need proxies for Tier 3 spot-check validation to avoid IP bans
        from real estate websites during scraping verification.
        """
        # For mainnet (netuid 13), require proxy configuration
        if self.config.netuid == 13:  # Mainnet
            if not hasattr(self.config, 'proxy_url') or not self.config.proxy_url:
                bt.logging.error("❌ PROXY REQUIRED: Mainnet validators must configure proxy for spot-check validation")
                bt.logging.error("   Add: --proxy_url http://username:password@proxy-server:port")
                bt.logging.error("   Recommended: Webshare.io Rotating Residential Proxies")
                bt.logging.error("   Plan: 100GB+ for reliable spot-checking")
                raise ValueError("Proxy configuration required for mainnet validators")
            
            bt.logging.success(f"✅ Proxy configured: {self.config.proxy_url.split('@')[-1] if '@' in self.config.proxy_url else self.config.proxy_url}")
            
        # For testnet, proxy is optional but recommended
        elif self.config.netuid == 428:  # Testnet
            if hasattr(self.config, 'proxy_url') and self.config.proxy_url:
                bt.logging.info(f"🔄 Testnet proxy configured: {self.config.proxy_url.split('@')[-1] if '@' in self.config.proxy_url else self.config.proxy_url}")
            else:
                bt.logging.warning("⚠️  No proxy configured for testnet - may encounter rate limits during spot-checks")
                bt.logging.info("   Consider adding: --proxy_url for production-like testing")

    def _configure_scraper_proxy(self):
        """
        Configure proxy settings for the validator's scraper components
        
        This ensures that Tier 3 spot-check validation uses the proxy
        to avoid IP bans from real estate websites.
        """
        try:
            import os
            
            # Set proxy environment variables for scrapers
            proxy_url = self.config.proxy_url
            
            # Handle proxy with authentication
            if hasattr(self.config, 'proxy_username') and self.config.proxy_username:
                if hasattr(self.config, 'proxy_password') and self.config.proxy_password:
                    # Format: http://username:password@proxy:port
                    if '://' in proxy_url:
                        protocol, rest = proxy_url.split('://', 1)
                        proxy_url = f"{protocol}://{self.config.proxy_username}:{self.config.proxy_password}@{rest}"
                    else:
                        proxy_url = f"http://{self.config.proxy_username}:{self.config.proxy_password}@{proxy_url}"
            
            # Set environment variables for HTTP requests
            os.environ['HTTP_PROXY'] = proxy_url
            os.environ['HTTPS_PROXY'] = proxy_url
            os.environ['http_proxy'] = proxy_url
            os.environ['https_proxy'] = proxy_url
            
            bt.logging.success("✅ Scraper proxy configuration applied")
            
        except Exception as e:
            bt.logging.error(f"Failed to configure scraper proxy: {e}")
            bt.logging.warning("Continuing without proxy - may encounter rate limits")

    def setup(self):
        """A one-time setup method that must be called before the Validator starts its main loop."""
        assert not self.is_setup, "Validator already setup."

        if self.config.wandb.on or not self.config.wandb.off:
            try:
                self.new_wandb_run()
            except Exception as e:
                bt.logging.exception("W&B init failed; will retry later.")
                # Do NOT flip wandb.off here; just remember there is no active run.
                self.wandb_run = None
                self.wandb_run_start = None

        metrics.VALIDATOR_INFO.info({
            "hotkey": self.wallet.hotkey.ss58_address,
            "uid": str(self.uid),
            "netuid": str(self.config.netuid),
            "version": self.get_version_tag(),
        })

        # Load any state from previous runs.
        self.load_state()

        # Getting latest dynamic lookup
        self.get_updated_lookup()

        # TODO: Configure this to expose access to data to neurons on certain subnets.
        # Serve axon to enable external connections.
        if not self.config.neuron.axon_off:
            self.serve_axon()
        else:
            bt.logging.warning("Axon off, not serving ip to chain.")

        self.organic_processor = OrganicQueryProcessor(
            wallet=self.wallet,
            metagraph=self.metagraph,
            evaluator=self.evaluator
        )
        
        self.is_setup = True

    def get_updated_lookup(self):
        try:
            t_start = time.perf_counter()
            bt.logging.info("Retrieving the latest dynamic lookup...")
            model = sync_run_retrieval(self.config)
            bt.logging.info("Model retrieved, updating value calculator...")
            self.evaluator.scorer.value_calculator = DataValueCalculator(model=model)
            bt.logging.info(f"Evaluator: {self.evaluator.scorer.value_calculator}")
            bt.logging.info(f"Updated dynamic lookup at {dt.datetime.utcnow()}")

            duration = time.perf_counter() - t_start

            metrics.DYNAMIC_DESIRABILITY_RETRIEVAL_PROCESS_DURATION.labels(hotkey=self.wallet.hotkey.ss58_address).set(duration)
            metrics.DYNAMIC_DESIRABILITY_RETRIEVAL_LAST_SUCCESSFUL_TS.labels(hotkey=self.wallet.hotkey.ss58_address).set(int(time.time()))
        except Exception as e:
            bt.logging.error(f"Error in get_updated_lookup: {str(e)}")
            bt.logging.exception("Exception details:")

    def get_version_tag(self):
        """Fetches version tag"""
        try:
            subprocess.run(['git', 'fetch', '--tags'], check=True)
            version_tag = subprocess.check_output(['git', 'describe', '--tags', '--abbrev=0']).strip().decode('utf-8')
            return version_tag
        
        except subprocess.CalledProcessError as e:
            print(f"Couldn't fetch latest version tag: {e}")
            return "error"
        
    def get_scraper_providers(self):
        """Fetches a validator's scraper providers to display in WandB logs."""
        scrapers = self.evaluator.PREFERRED_SCRAPERS
        return scrapers

    def new_wandb_run(self):
        """Creates a new wandb run to save information to."""
        # Create a unique run id for this run.
        now = dt.datetime.now()
        run_id = now.strftime("%Y-%m-%d_%H-%M-%S")
        name = "validator-" + str(self.uid) + "-" + run_id
        version_tag = self.get_version_tag()
        scraper_providers = self.get_scraper_providers()
    
        # Allow multiple runs in one process and only set start time after success
        self.wandb_run = wandb.init(
            name=name,
            project="resi-validators",
            entity="resi-labs-ai",
            config={
                "uid": self.uid,
                "hotkey": self.wallet.hotkey.ss58_address,
                "run_name": run_id,
                "type": "validator",
                "version": version_tag,
                "scrapers": scraper_providers
            },
            allow_val_change=True,
            anonymous="allow",
            reinit=True,
            resume="never", # force a brand-new run ID on each rotation
            settings=wandb.Settings(start_method="thread"),
        )
    
        # Start time after successful init so rotation scheduling is correct
        self.wandb_run_start = now
    
        bt.logging.debug(f"Started a new wandb run: {name}")


    def run(self):
        """
        Initiates and manages the main loop for the validator, which
    
        1. Evaluates miners
        2. Periodically writes the latest scores to the chain
        3. Saves state
        """
        assert self.is_setup, "Validator must be setup before running."
    
        # Check that validator is registered on the network.
        utils.assert_registered(self.wallet, self.metagraph)
    
        bt.logging.info(
            f"Running validator {self.axon} on network: {self.config.subtensor.chain_endpoint} with netuid: {self.config.netuid}."
        )
    
        bt.logging.info(f"Validator starting at block: {self.block}.")
    
        while not self.should_exit:
            try:
                t_start = time.perf_counter()
    
                bt.logging.debug(
                    f"Validator running on step({self.step}) block({self.block})."
                )
    
                # Ensure validator hotkey is still registered on the network.
                utils.assert_registered(self.wallet, self.metagraph)
    
                # Evaluate the next batch of miners.
                next_batch_delay_secs = self.loop.run_until_complete(
                    self.evaluator.run_next_eval_batch()
                )
                self._on_eval_batch_complete()
    
                # Maybe set weights.
                if self.should_set_weights():
                    self.set_weights()
    
                # Always save state.
                self.save_state()
    
                # Update to the latest desirability list after each evaluation.
                self.get_updated_lookup()
    
                self.step += 1
    
                metrics.MAIN_LOOP_ITERATIONS_TOTAL.labels(hotkey=self.wallet.hotkey.ss58_address).inc()
                metrics.MAIN_LOOP_LAST_SUCCESS_TS.labels(hotkey=self.wallet.hotkey.ss58_address).set(int(time.time()))
                metrics.MAIN_LOOP_DURATION.labels(hotkey=self.wallet.hotkey.ss58_address).set(time.perf_counter() - t_start)
    
                wait_time = max(0.0, float(next_batch_delay_secs))
                if wait_time > 0:
                    bt.logging.info(
                        f"Finished full evaluation loop early. Waiting {wait_time} seconds until running next evaluation loop."
                    )
                    time.sleep(wait_time)
    
                if (self.config.wandb.on or not self.config.wandb.off) and self.wandb_run is None:
                    try:
                        self.new_wandb_run()
                        bt.logging.info("W&B: started new run successfully")
                    except Exception as e:
                        bt.logging.error(f"W&B init retry failed: {e}")

                # Rotation with retry (only when we actually have a start time)
                if (self.config.wandb.on or not self.config.wandb.off) and (self.wandb_run_start is not None) and \
                ((dt.datetime.now() - self.wandb_run_start) >= dt.timedelta(hours=3)):

                    try:
                        self.new_wandb_run()
                        bt.logging.info("W&B: rotated run successfully")
                    except Exception as e:
                        bt.logging.error(f"W&B rotation failed; keeping current run active: {e}")

            except KeyboardInterrupt:
                self.axon.stop()
                if self.wandb_run:
                    self.wandb_run.finish()
            except Exception as err:
                metrics.MAIN_LOOP_ERRORS_TOTAL.labels(hotkey=self.wallet.hotkey.ss58_address).inc()
                bt.logging.error("Error during validation", str(err))

    def run_in_background_thread(self):
        """
        Starts the validator's operations in a background thread upon entering the context.
        This method facilitates the use of the validator in a 'with' statement.
        """

        # Setup the Validator.
        self.setup()

        if not self.is_running:
            bt.logging.debug("Starting validator in background thread.")
            self.should_exit = False
            self.thread = threading.Thread(target=self.run, daemon=True)
            self.thread.start()
            self.is_running = True
            bt.logging.debug("Started.")

    def stop_run_thread(self):
        """
        Stops the validator's operations that are running in the background thread.
        """
        if self.is_running:
            bt.logging.debug("Stopping validator in background thread.")
            self.should_exit = True
            self.thread.join(5)
            self.is_running = False
            bt.logging.debug("Stopped.")

    def __enter__(self):
        self.run_in_background_thread()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        """
        Stops the validator's background operations upon exiting the context.
        This method facilitates the use of the validator in a 'with' statement.

        Args:
            exc_type: The type of the exception that caused the context to be exited.
                      None if the context was exited without an exception.
            exc_value: The instance of the exception that caused the context to be exited.
                       None if the context was exited without an exception.
            traceback: A traceback object encoding the stack trace.
                       None if the context was exited without an exception.
        """
        if self.is_running:
            bt.logging.debug("Stopping validator in background thread.")
            self.should_exit = True
            self.thread.join(5)
            self.is_running = False

            # Cleanup loggers
            if self.wandb_run:
                self.wandb_run.finish()
            bt.logging.debug("Stopped.")

    def serve_axon(self):
        """Serve axon to enable external connections."""

        try:
            self.axon = bt.axon(wallet=self.wallet, config=self.config)
            # Import and attach organic synapse
            self.axon.attach(
                forward_fn=self.process_organic_query,
                blacklist_fn=self.organic_blacklist,
                priority_fn=self.organic_priority
            )

            self.axon.serve(netuid=self.config.netuid, subtensor=self.subtensor).start()
            if self.config.neuron.api_on:
                try:
                    bt.logging.info("Starting Validator API...")
                    from vali_utils.api.server import ValidatorAPI
                    self.api = ValidatorAPI(self, port=self.config.neuron.api_port)
                    self.api.start()
                    # Start monitoring to auto-restart if it fails
                    self._start_api_monitoring()

                except ValueError as e:
                    bt.logging.error(f"Failed to start API: {str(e)}")
                    bt.logging.info("Validator will continue running without API.")
                    self.config.neuron.api_on = False

            bt.logging.info(
                f"Serving validator axon {self.axon} on network: {self.config.subtensor.chain_endpoint} with netuid: {self.config.netuid}."
            )
        except Exception as e:
            bt.logging.error(f"Failed to setup Axon: {e}.")
            sys.exit(1)

    def _on_metagraph_updated(self, metagraph: bt.metagraph, netuid: int):
        """Processes an update to the metagraph"""
        with self.lock:
            assert netuid == self.config.netuid
            self.metagraph = copy.deepcopy(metagraph)

        # Validator Health Checks
        hotkey = self.wallet.hotkey.ss58_address

        # Resolve current UID from hotkey (survives dereg/re-reg)
        try:
            uid = metagraph.hotkeys.index(hotkey)
            registered = 1
        except ValueError:
            uid = None
            registered = 0

        metrics.BITTENSOR_VALIDATOR_REGISTERED.labels(hotkey=hotkey).set(registered)

        # Set vtrust + block diff since last update only when registered; otherwise zero them
        if registered:
            try:
                vtrust = float(metagraph.validator_trust[uid])
            except Exception:
                vtrust = 0.0

            try:
                head_block = int(getattr(metagraph, "block", 0))
                last_update = int(metagraph.last_update[uid])
                block_diff = max(0, head_block - last_update) if head_block and last_update else 0
            except Exception:
                block_diff = 0
        else:
            vtrust = 0.0
            block_diff = 0

        metrics.BITTENSOR_VALIDATOR_VTRUST.labels(hotkey=hotkey).set(vtrust)
        metrics.BITTENSOR_VALIDATOR_BLOCK_DIFFERENCE.labels(hotkey=hotkey).set(block_diff)
        metrics.METAGRAPH_LAST_UPDATE_TS.labels(hotkey=hotkey).set(int(time.time()))

    def _on_eval_batch_complete(self):
        with self.lock:
            self.evaluation_cycles_since_startup += 1
            self.last_eval_time = dt.datetime.utcnow()

    def is_healthy(self) -> bool:
        """Returns true if the validator is healthy and is evaluating Miners."""
        with self.lock:
            return dt.datetime.utcnow() - self.last_eval_time < dt.timedelta(minutes=35)

    def should_set_weights(self) -> bool:
        # Check if enough epoch blocks have elapsed since the last epoch.
        if self.config.neuron.disable_set_weights:
            return False

        with self.lock:
            # After a restart, we want to wait two evaluation cycles
            # Check if we've completed at least two evaluation cycles since startup
            if not self.evaluation_cycles_since_startup:
                self.evaluation_cycles_since_startup = 0
                bt.logging.info("Initializing evaluation cycles counter for delayed weight setting")

            #  if we've completed fewer than the allotted number of evaluation cycles, don't set weights
            if self.evaluation_cycles_since_startup < constants.EVALUATION_ON_STARTUP:

                bt.logging.info(
                    f"Skipping weight setting - completed {self.evaluation_cycles_since_startup}/15 evaluation cycles since startup")
                return False

            # Check if an epoch has passed since last weight setting
            current_block = self.block
            epoch_length = self.config.neuron.epoch_length
            blocks_since_last_weights = current_block - self.last_weights_set_block
            
            # Log current status for debugging
            bt.logging.debug(f"Weight setting check: current_block={current_block}, last_weights_set_block={self.last_weights_set_block}, blocks_since_last_weights={blocks_since_last_weights}, epoch_length={epoch_length}")
            
            # If we haven't set weights yet, or if an epoch has passed
            if self.last_weights_set_block == 0 or blocks_since_last_weights >= epoch_length:
                bt.logging.info(f"Epoch boundary reached: current_block={current_block}, last_weights_set_block={self.last_weights_set_block}, blocks_since_last_weights={blocks_since_last_weights}, epoch_length={epoch_length}")
                return True
            
            return False

    def _start_api_monitoring(self):
        """Start a lightweight monitor to auto-restart API if it becomes unreachable"""

        master_key = os.getenv('MASTER_KEY')

        def monitor_api():
            consecutive_failures = 0
            max_failures = 3  # Restart after 3 consecutive failures

            while not self.should_exit:
                if not hasattr(self, 'api') or not self.api:
                    time.sleep(60 * 20)
                    continue

                try:
                    # Try a simple local request to check if API is responding
                    response = requests.get(
                        f"http://localhost:{self.config.neuron.api_port}/api/v1/monitoring/system-status",
                        headers={"X-API-Key": master_key},
                        timeout=10
                    )

                    if response.status_code == 200:
                        # API is working, reset failure counter
                        consecutive_failures = 0
                    else:
                        # HTTP error
                        consecutive_failures += 1
                        bt.logging.warning(f"API health check returned status {response.status_code}")
                except requests.RequestException:
                    # Connection error (most likely API is down)
                    consecutive_failures += 1
                    bt.logging.warning(f"API server not responding ({consecutive_failures}/{max_failures})")

                # If too many consecutive failures, restart API
                if consecutive_failures >= max_failures:
                    bt.logging.warning(f"API server unresponsive for {consecutive_failures} checks, restarting...")

                    try:
                        # Stop API if it's running
                        if hasattr(self.api, 'stop'):
                            self.api.stop()

                        # Wait a moment
                        time.sleep(2)

                        # Start API again
                        if hasattr(self.api, 'start'):
                            self.api.start()

                        bt.logging.info("API server restarted")
                        consecutive_failures = 0
                    except Exception as e:
                        bt.logging.error(f"Error restarting API server: {str(e)}")

                # Check every 30 minutes
                time.sleep(30 * 60)

        # Start monitoring in background thread
        thread = threading.Thread(target=monitor_api, daemon=True)
        thread.start()
        bt.logging.info("API monitoring started")

    def set_weights(self):
        """
        Sets the validator weights to the metagraph hotkeys based on the scores it has received from the miners. The weights determine the trust and incentive level the validator assigns to miner nodes on the network.
        """
        bt.logging.info("Attempting to set weights.")

        scorer = self.evaluator.get_scorer()
        scores = scorer.get_scores()
        credibilities = scorer.get_credibilities()

        # Check if scores contains any NaN values and log a warning if it does.
        if torch.isnan(scores).any():
            bt.logging.warning(
                f"Scores contain NaN values. This may be due to a lack of responses from miners, or a bug in your reward functions."
            )

        # Calculate the average reward for each uid across non-zero values.
        # Replace any NaN values with 0.
        raw_weights = torch.nn.functional.normalize(scores, p=1, dim=0)

        # Process the raw weights to final_weights via subtensor limitations.
        (
            processed_weight_uids,
            processed_weights,
        ) = bt.utils.weight_utils.process_weights_for_netuid(
            uids=torch.tensor(self.metagraph.uids, dtype=torch.int64).to("cpu"),
            weights=raw_weights.detach().cpu().numpy().astype(np.float32),
            netuid=self.config.netuid,
            subtensor=self.subtensor,
            metagraph=self.metagraph,
        )

        table = Table(title="All Weights")
        table.add_column("uid", justify="right", style="cyan", no_wrap=True)
        table.add_column("weight", style="magenta")
        table.add_column("score", style="magenta")
        table.add_column("credibility", style="magenta")
        uids_and_weights = list(
            zip(processed_weight_uids.tolist(), processed_weights.tolist())
        )
        # Sort by weights descending.
        sorted_uids_and_weights = sorted(
            uids_and_weights, key=lambda x: x[1], reverse=True
        )
        for uid, weight in sorted_uids_and_weights:
            table.add_row(
                str(uid),
                str(round(weight, 4)),
                str(int(scores[uid].item())),
                str(round(credibilities[uid].item(), 4)),
            )
        console = Console()
        console.print(table)


        # Set the weights on chain via our subtensor connection.
        t0 = time.perf_counter()
        success, message = self.subtensor.set_weights(
            wallet=self.wallet,
            netuid=self.config.netuid,
            uids=processed_weight_uids,
            weights=processed_weights,
            wait_for_finalization=False,
            version_key=spec_version,
        )
        
        with self.lock:
            self.last_weights_set_time = dt.datetime.utcnow()
            self.last_weights_set_block = self.block

        bt.logging.success("Finished setting weights.")

        metric_status = 'ok' if success else 'fail'
        metrics.SET_WEIGHTS_SUBTENSOR_DURATION.labels(hotkey=self.wallet.hotkey.ss58_address, status=metric_status).observe(time.perf_counter() - t0)
        metrics.SET_WEIGHTS_LAST_TS.labels(hotkey=self.wallet.hotkey.ss58_address, status=metric_status).set(int(time.time()))

    @property
    def block(self):
        return utils.ttl_get_block(self)

    def save_state(self):
        """Saves the state of the validator to a file."""
        bt.logging.trace("Saving validator state.")

        self.evaluator.save_state()

    def load_state(self):
        """Loads the state of the validator from a file."""
        bt.logging.info("Loading validator state.")

        self.evaluator.load_state()

    async def process_organic_query(self, synapse: OrganicRequest) -> OrganicRequest:
        """
        Process organic queries through the validator axon.
        Delegates to OrganicQueryProcessor for actual processing.
        """
        if not self.organic_processor:
            synapse.status = "error"
            synapse.meta = {"error": "Organic query processor not initialized"}
            synapse.data = []
            return synapse
        
        t_start = time.perf_counter() 
        self.organic_processor.update_metagraph(self.evaluator.metagraph)
        synapse_resp = await self.organic_processor.process_organic_query(synapse)

        metrics.ORGANIC_QUERY_PROCESS_DURATION.labels(request_source=synapse.source, response_status=synapse_resp.status).observe(time.perf_counter() - t_start)

        try:
            json_str = json.dumps(synapse_resp.data)
            size_bytes = len(json_str.encode('utf-8'))

            metrics.ORGANIC_QUERY_RESPONSE_SIZE.labels(request_source=synapse.source, response_status=synapse_resp.status).observe(size_bytes)
        except (TypeError, ValueError) as e:  # JSON serialization errors
            bt.logging.debug("Failed to serialize synapse response data to JSON. Skipping metrics.ORGANIC_QUERY_RESPONSE_BYTES observation")

        metrics.ORGANIC_QUERY_REQUESTS_TOTAL.labels(request_source=synapse.source, response_status=synapse_resp.status).inc()
        return synapse_resp

    async def organic_blacklist(self, synapse: OrganicRequest) -> Tuple[bool, str]:
        """
        Simplified blacklist function that only checks whitelist membership
        """
        # Only allow hotkeys in the whitelist
        if hasattr(self.config, 'organic_whitelist') and self.config.organic_whitelist:
            if synapse.dendrite.hotkey in self.config.organic_whitelist:
                return False, "Request accepted from whitelisted hotkey"
            else:
                return True, f"Sender {synapse.dendrite.hotkey} not in whitelist"

        # If no whitelist is defined, reject all requests
        return True, "No whitelist configured"

    def organic_priority(self, synapse: OrganicRequest) -> float:
        caller_uid = self.metagraph.hotkeys.index(synapse.dendrite.hotkey)
        priority = float(self.metagraph.S[caller_uid])
        bt.logging.trace(
            f"Prioritizing {synapse.dendrite.hotkey} with value: {priority}.",
        )
        return priority
    
    def run_epoch_validation(self):
        """
        Deterministic epoch validation ensuring all validators reach same results
        
        This method:
        1. Waits for epoch completion
        2. Downloads miner submissions from S3
        3. Processes each zipcode with multi-tier validation
        4. Calculates proportional weights
        5. Verifies consensus with other validators
        6. Updates Bittensor weights
        """
        if not self.zipcode_validation_enabled or not self.api_client:
            bt.logging.warning("Zipcode validation not enabled")
            return
        
        bt.logging.info("Starting epoch validation cycle...")
        
        while not self.should_exit:
            try:
                # Wait for epoch to complete
                current_epoch = self.api_client.get_current_epoch_info()
                
                if not current_epoch:
                    bt.logging.info("No current epoch available, waiting...")
                    time.sleep(300)  # Wait 5 minutes
                    continue
                
                epoch_id = current_epoch.get('id')
                epoch_status = current_epoch.get('status')
                
                # Check if epoch is completed or near completion
                if epoch_status == 'ACTIVE':
                    # Wait for epoch to complete
                    time_until_end = current_epoch.get('timeUntilEpochEnd', 0)
                    if time_until_end > 300:  # More than 5 minutes left
                        bt.logging.info(f"Epoch {epoch_id} still active, waiting {time_until_end}s")
                        time.sleep(min(300, time_until_end))
                        continue
                
                # Check if this is a new epoch we haven't processed
                if (not self.current_epoch_data or 
                    self.current_epoch_data.get('epoch_id') != epoch_id):
                    
                    bt.logging.info(f"Processing completed epoch: {epoch_id}")
                    
                    # Download epoch submissions by zipcode
                    zipcode_submissions = self.download_epoch_submissions_by_zipcode(epoch_id)
                    
                    if not zipcode_submissions:
                        bt.logging.warning(f"No submissions found for epoch {epoch_id}")
                        time.sleep(300)
                        continue
                    
                    # Get epoch nonce for deterministic validation
                    epoch_nonce = current_epoch.get('nonce', epoch_id)
                    
                    # Process each zipcode with multi-tier validation
                    all_zipcode_results = []
                    
                    for zipcode, submissions in zipcode_submissions.items():
                        bt.logging.info(f"Validating {len(submissions)} submissions for zipcode {zipcode}")
                        
                        # Get expected listings for this zipcode
                        expected_listings = self.get_expected_listings_for_zipcode(zipcode, epoch_id)
                        
                        # Validate and rank submissions for this zipcode
                        zipcode_result = self.zipcode_scorer.validate_and_rank_zipcode_submissions(
                            zipcode=zipcode,
                            submissions=submissions,
                            expected_listings=expected_listings,
                            epoch_nonce=epoch_nonce
                        )
                        
                        all_zipcode_results.append(zipcode_result)
                    
                    # Calculate final proportional weights across all zipcodes
                    final_scores = self.zipcode_scorer.calculate_epoch_proportional_weights(all_zipcode_results)
                    
                    # Calculate consensus hash for verification
                    consensus_hash = self.consensus_manager.calculate_consensus_hash(final_scores, epoch_nonce)
                    
                    # Create validation result with consensus data
                    validation_result = self.consensus_manager.create_validation_result_with_consensus(
                        epoch_id=epoch_id,
                        final_scores=final_scores,
                        zipcode_results=all_zipcode_results,
                        epoch_nonce=epoch_nonce
                    )
                    validation_result['validator_hotkey'] = self.wallet.hotkey.ss58_address
                    
                    # Upload validation results to S3
                    self.upload_validation_results(validation_result)
                    
                    # Verify consensus across validators
                    consensus_result = self.consensus_manager.verify_consensus_across_validators(
                        epoch_id, consensus_hash
                    )
                    
                    # Handle consensus verification
                    if consensus_result['consensus_status'] in ['PERFECT_CONSENSUS', 'MAJORITY_CONSENSUS']:
                        bt.logging.success(f"Consensus achieved: {consensus_result['consensus_status']}")
                        
                        # Update Bittensor weights with final scores
                        self.update_bittensor_weights_from_zipcode_scores(final_scores['miner_scores'])
                        
                        # Store epoch data for next iteration
                        self.current_epoch_data = {
                            'epoch_id': epoch_id,
                            'consensus_hash': consensus_hash,
                            'final_scores': final_scores
                        }
                        
                    else:
                        bt.logging.error(f"Consensus failed: {consensus_result['consensus_status']}")
                        self.consensus_manager.handle_consensus_failure(epoch_id, consensus_result)
                
                # Wait before checking for next epoch
                time.sleep(300)  # Check every 5 minutes
                
            except Exception as e:
                bt.logging.error(f"Error in epoch validation cycle: {e}")
                bt.logging.error(f"Traceback: {traceback.format_exc()}")
                time.sleep(300)  # Wait 5 minutes on error
    
    def download_epoch_submissions_by_zipcode(self, epoch_id: str) -> dict:
        """
        Download and organize miner submissions by zipcode for an epoch
        
        Args:
            epoch_id: Epoch ID to download submissions for
            
        Returns:
            Dict mapping zipcode -> list of miner submissions
        """
        try:
            bt.logging.info(f"Downloading submissions for epoch {epoch_id}")
            
            # Get list of active miners from metagraph
            active_miners = []
            for uid, hotkey in enumerate(self.evaluator.metagraph.hotkeys):
                if self.evaluator.metagraph.S[uid] > 0:  # Has stake
                    active_miners.append(hotkey)
            
            bt.logging.info(f"Checking {len(active_miners)} active miners for epoch {epoch_id} submissions")
            
            # Download submissions from each miner
            zipcode_submissions = {}
            successful_downloads = 0
            
            for miner_hotkey in active_miners:
                try:
                    miner_submissions = self._download_miner_epoch_data(epoch_id, miner_hotkey)
                    
                    if miner_submissions:
                        successful_downloads += 1
                        
                        # Organize by zipcode
                        for zipcode, submission_data in miner_submissions.items():
                            if zipcode not in zipcode_submissions:
                                zipcode_submissions[zipcode] = []
                            
                            # Add miner info to submission
                            submission_data['miner_hotkey'] = miner_hotkey
                            zipcode_submissions[zipcode].append(submission_data)
                            
                        bt.logging.debug(f"Downloaded {len(miner_submissions)} zipcode submissions from {miner_hotkey[:8]}...")
                    
                except Exception as miner_error:
                    bt.logging.debug(f"No epoch data from miner {miner_hotkey[:8]}...: {miner_error}")
                    continue
            
            bt.logging.info(f"Downloaded epoch {epoch_id} data from {successful_downloads} miners across {len(zipcode_submissions)} zipcodes")
            
            return zipcode_submissions
            
        except Exception as e:
            bt.logging.error(f"Failed to download epoch submissions: {e}")
            return {}
    
    def _download_miner_epoch_data(self, epoch_id: str, miner_hotkey: str) -> dict:
        """
        Download epoch data from a specific miner
        
        Args:
            epoch_id: Epoch ID
            miner_hotkey: Miner's hotkey
            
        Returns:
            Dict mapping zipcode -> submission data
        """
        try:
            # Use existing ValidatorS3Access to get miner data
            miner_url = asyncio.run(self.s3_reader.get_miner_specific_access(miner_hotkey))
            
            if not miner_url:
                return {}
            
            # Download and parse S3 file list
            import requests
            response = requests.get(miner_url, timeout=30)
            
            if response.status_code != 200:
                return {}
            
            # Parse S3 XML to find epoch-specific files
            epoch_files = self._parse_epoch_files_from_s3_xml(response.text, epoch_id, miner_hotkey)
            
            if not epoch_files:
                return {}
            
            # Download and parse each epoch file
            miner_epoch_data = {}
            
            for file_info in epoch_files:
                try:
                    file_data = self._download_and_parse_epoch_file(file_info)
                    
                    if file_data and 'zipcode' in file_data:
                        zipcode = file_data['zipcode']
                        miner_epoch_data[zipcode] = file_data
                        
                except Exception as file_error:
                    bt.logging.debug(f"Failed to download epoch file {file_info.get('key', 'unknown')}: {file_error}")
                    continue
            
            return miner_epoch_data
            
        except Exception as e:
            bt.logging.debug(f"Error downloading miner epoch data from {miner_hotkey[:8]}...: {e}")
            return {}
    
    def _parse_epoch_files_from_s3_xml(self, xml_content: str, epoch_id: str, miner_hotkey: str) -> list:
        """
        Parse S3 XML response to find epoch-specific files
        
        Args:
            xml_content: S3 XML response
            epoch_id: Target epoch ID
            miner_hotkey: Miner's hotkey
            
        Returns:
            List of epoch file info
        """
        try:
            import xml.etree.ElementTree as ET
            
            root = ET.fromstring(xml_content)
            namespace = {'s3': 'http://s3.amazonaws.com/doc/2006-03-01/'}
            
            epoch_files = []
            
            for content in root.findall('.//s3:Contents', namespace):
                key_elem = content.find('s3:Key', namespace)
                size_elem = content.find('s3:Size', namespace)
                modified_elem = content.find('s3:LastModified', namespace)
                
                if key_elem is not None:
                    key = key_elem.text
                    
                    # Look for epoch-specific files
                    # Expected pattern: data/hotkey={miner_hotkey}/epoch={epoch_id}/zipcode={zipcode}/data_*.json
                    if (f'epoch={epoch_id}' in key and 
                        f'hotkey={miner_hotkey}' in key and 
                        key.endswith('.json')):
                        
                        # Extract zipcode from path
                        zipcode = None
                        if 'zipcode=' in key:
                            zipcode_part = key.split('zipcode=')[1].split('/')[0]
                            zipcode = zipcode_part
                        
                        file_info = {
                            'key': key,
                            'size': int(size_elem.text) if size_elem is not None else 0,
                            'last_modified': modified_elem.text if modified_elem is not None else '',
                            'zipcode': zipcode,
                            'download_url': f"{miner_url.split('?')[0]}/{key}?{miner_url.split('?')[1]}" if '?' in miner_url else f"{miner_url}/{key}"
                        }
                        
                        epoch_files.append(file_info)
            
            return epoch_files
            
        except Exception as e:
            bt.logging.error(f"Error parsing S3 XML for epoch files: {e}")
            return []
    
    def _download_and_parse_epoch_file(self, file_info: dict) -> dict:
        """
        Download and parse an individual epoch file
        
        Args:
            file_info: File information from S3
            
        Returns:
            Parsed file data
        """
        try:
            import requests
            import json
            
            # Download the file
            response = requests.get(file_info['download_url'], timeout=30)
            
            if response.status_code != 200:
                return {}
            
            # Parse JSON data
            file_data = json.loads(response.text)
            
            # Validate required fields
            required_fields = ['epoch_id', 'zipcode', 'miner_hotkey', 'listings']
            if not all(field in file_data for field in required_fields):
                bt.logging.warning(f"Epoch file missing required fields: {file_info['key']}")
                return {}
            
            # Add file metadata
            file_data['file_info'] = {
                'key': file_info['key'],
                'size': file_info['size'],
                'last_modified': file_info['last_modified']
            }
            
            return file_data
            
        except Exception as e:
            bt.logging.error(f"Error downloading epoch file {file_info.get('key', 'unknown')}: {e}")
            return {}
    
    def get_expected_listings_for_zipcode(self, zipcode: str, epoch_id: str) -> int:
        """
        Get expected number of listings for a zipcode in an epoch
        
        Args:
            zipcode: Target zipcode
            epoch_id: Epoch ID
            
        Returns:
            Expected number of listings
        """
        try:
            # First, check if we have cached epoch assignment data
            if (self.current_epoch_data and 
                self.current_epoch_data.get('epoch_id') == epoch_id):
                
                for zipcode_info in self.current_epoch_data.get('zipcodes', []):
                    if zipcode_info['zipcode'] == zipcode:
                        expected = zipcode_info['expectedListings']
                        bt.logging.debug(f"Found cached expected listings for {zipcode}: {expected}")
                        return expected
            
            # If not cached or different epoch, try to get current assignments from API
            if self.api_client:
                try:
                    assignments = self.api_client.get_current_zipcode_assignments()
                    
                    if assignments.get('success') and assignments.get('epochId') == epoch_id:
                        # Cache the assignments for future use
                        self.current_epoch_data = {
                            'epoch_id': epoch_id,
                            'zipcodes': assignments.get('zipcodes', []),
                            'nonce': assignments.get('nonce'),
                            'cached_at': datetime.now(timezone.utc).isoformat()
                        }
                        
                        # Find the specific zipcode
                        for zipcode_info in assignments.get('zipcodes', []):
                            if zipcode_info['zipcode'] == zipcode:
                                expected = zipcode_info['expectedListings']
                                bt.logging.info(f"Retrieved expected listings for {zipcode}: {expected}")
                                return expected
                        
                        bt.logging.warning(f"Zipcode {zipcode} not found in current epoch assignments")
                    else:
                        bt.logging.warning(f"API returned assignments for different epoch or failed")
                        
                except Exception as api_error:
                    bt.logging.error(f"Failed to get zipcode assignments from API: {api_error}")
            
            # Fallback: Use historical average or reasonable default based on zipcode
            default_expected = self._get_default_expected_listings(zipcode)
            bt.logging.warning(f"Using default expected listings for {zipcode}: {default_expected}")
            return default_expected
            
        except Exception as e:
            bt.logging.error(f"Failed to get expected listings for {zipcode}: {e}")
            return 250  # Ultimate fallback
    
    def _get_default_expected_listings(self, zipcode: str) -> int:
        """
        Get reasonable default expected listings based on zipcode characteristics
        
        This could be enhanced with historical data or zipcode population data
        """
        try:
            # For now, use simple heuristics based on zipcode patterns
            # This could be replaced with actual historical data
            
            # Major metropolitan areas (rough approximation)
            major_metro_patterns = [
                '100', '101', '102',  # NYC area
                '900', '901', '902',  # LA area  
                '606', '607', '608',  # Chicago area
                '770', '771', '772',  # Atlanta area
                '191', '192', '193',  # Philadelphia area
            ]
            
            zipcode_prefix = zipcode[:3] if len(zipcode) >= 3 else zipcode
            
            if any(zipcode.startswith(pattern) for pattern in major_metro_patterns):
                return 400  # Higher expected for major metros
            elif zipcode_prefix in ['750', '751', '752', '753']:  # Texas
                return 350
            elif zipcode_prefix in ['330', '331', '332']:  # Florida
                return 300
            else:
                return 250  # Standard default
                
        except Exception as e:
            bt.logging.error(f"Error in default expected listings calculation: {e}")
            return 250
    
    def upload_validation_results(self, validation_result: dict):
        """
        Upload validation results to S3 for consensus verification
        
        Args:
            validation_result: Complete validation result with consensus data
        """
        try:
            epoch_id = validation_result['epoch_id']
            bt.logging.info(f"Uploading validation results for epoch {epoch_id}")
            
            # Get S3 credentials for validator upload
            s3_creds = self.api_client.get_validator_s3_credentials(
                epoch_id=epoch_id,
                purpose="epoch_validation_results"
            )
            
            if not s3_creds.get('success'):
                bt.logging.error("Failed to get validator S3 credentials")
                return False
            
            # TODO: Upload validation results using existing S3 infrastructure
            # This would use the existing ValidatorS3Access but for result uploads
            
            bt.logging.info("Validation result upload not yet implemented")
            return True
            
        except Exception as e:
            bt.logging.error(f"Failed to upload validation results: {e}")
            return False
    
    def update_bittensor_weights_from_zipcode_scores(self, miner_scores: dict):
        """
        Update Bittensor weights based on zipcode competitive scores
        
        Args:
            miner_scores: Dict mapping miner hotkey -> normalized score
        """
        try:
            bt.logging.info(f"Updating Bittensor weights for {len(miner_scores)} miners")
            
            # Convert miner scores to weight tensor
            metagraph = self.evaluator.metagraph
            weights = torch.zeros(len(metagraph.hotkeys))
            
            for hotkey, score in miner_scores.items():
                try:
                    uid = metagraph.hotkeys.index(hotkey)
                    weights[uid] = score
                except ValueError:
                    bt.logging.warning(f"Miner {hotkey[:8]} not found in metagraph")
                    continue
            
            # Normalize weights to sum to 1.0
            if weights.sum() > 0:
                weights = weights / weights.sum()
            
            bt.logging.info(f"Setting weights for {(weights > 0).sum()} miners")
            
            # Use existing weight setting mechanism
            self.set_weights(weights)
            
        except Exception as e:
            bt.logging.error(f"Failed to update Bittensor weights: {e}")


def main():
    """Main constructs the validator with its dependencies."""

    config = create_config(NeuronType.VALIDATOR)
    check_config(config=config)

    bt.logging(config=config, logging_dir=config.full_path)

    subtensor = bt.subtensor(config=config)
    metagraph = subtensor.metagraph(netuid=config.netuid)
    wallet = bt.wallet(config=config)

    # Get the wallet's UID, if registered.
    utils.assert_registered(wallet, metagraph)
    uid = utils.get_uid(wallet, metagraph)

    # Create the metagraph syncer and perform the initial sync.
    metagraph_syncer = MetagraphSyncer(subtensor, config={config.netuid: 20 * 60})
    # Perform an initial sync of all tracked metagraphs.
    metagraph_syncer.do_initial_sync()
    metagraph_syncer.start()

    s3_reader = ValidatorS3Access(wallet=wallet, s3_auth_url=config.s3_auth_url)
    evaluator = MinerEvaluator(
        config=config, uid=uid, metagraph_syncer=metagraph_syncer, s3_reader=s3_reader
    )

    with Validator(
        metagraph_syncer=metagraph_syncer,
        evaluator=evaluator,
        uid=uid,
        config=config,
        subtensor=subtensor,
    ) as validator:
        while True:
            if not validator.is_healthy():
                bt.logging.error("Validator is unhealthy. Restarting.")
                # Sys.exit() may not shutdown the process because it'll wait for other threads
                # to complete. Use os._exit() instead.
                os._exit(1)
            bt.logging.trace("Validator running...", time.time())
            time.sleep(60)


# The main function parses the configuration and runs the validator.
if __name__ == "__main__":
    main()
