# üéØ Bittensor Subnet 46: Validator & Miner Integration Plan for Zipcode-Based Real Estate Mining

## üìã **Executive Summary**

This document provides a comprehensive integration plan for upgrading the existing Bittensor Subnet 46 validator and miner code to work with the new NestJS API server that implements zipcode-based competitive real estate data mining. The new system introduces epoch-based zipcode assignments, S3 data storage, and validator-driven scoring mechanisms.

---

## üèóÔ∏è **System Architecture Overview**

### **Current State vs. New State**

#### **Before (Legacy System)**
- Miners scrape arbitrary real estate data
- No centralized coordination or assignments
- Manual data validation and scoring
- Limited anti-gaming mechanisms

#### **After (New Zipcode System)**
- **Epoch-Based Mining**: 4-hour competitive mining cycles
- **Centralized Zipcode Assignment**: API server assigns specific zipcodes to prevent overlap
- **S3 Data Storage**: Structured data upload and validation
- **Automated Validator Scoring**: Validators download, analyze, and score miner submissions
- **Anti-Gaming Features**: Honeypot zipcodes, nonce verification, signature authentication

---

## üåê **API Server Integration Details**

### **Base Configuration**
```bash
# Production API Server
BASE_URL: https://api.resilabs.com
SWAGGER_DOCS: https://api.resilabs.com/docs
HEALTH_CHECK: https://api.resilabs.com/healthcheck

# Staging environment
BASE_URL: https://api-staging.resilabs.com
SWAGGER_DOCS: https://api-staging.resilabs.com/docs
HEALTH_CHECK: https://api-staging.resilabs.com/healthcheck

# Development/Testing
BASE_URL: http://localhost:3000
```

### **Authentication System**
All API calls require Bittensor hotkey signatures with specific commitment formats:

```python
# Signature Generation Template
def generate_signature(hotkey, commitment_string):
    commitment_bytes = commitment_string.encode('utf-8')
    signature = hotkey.sign(commitment_bytes).hex()
    return signature

# Commitment Formats by Endpoint
MINER_ZIPCODE_REQUEST = "zipcode:assignment:current:{timestamp}"
MINER_STATUS_UPDATE = "miner:status:{hotkey}:{epochId}:{timestamp}"
MINER_S3_ACCESS = "s3:data:access:{coldkey}:{hotkey}:{timestamp}"
VALIDATOR_S3_ACCESS = "s3:validator:upload:{timestamp}"
```

---

## üó∫Ô∏è **Core API Endpoints for Integration**

### **1. Miner Zipcode Assignment**
```http
GET /api/v1/zipcode-assignments/current?hotkey={hotkey}&signature={signature}&timestamp={timestamp}
```

**Purpose**: Get current epoch's zipcode assignments for competitive mining

**Response Structure**:
```json
{
  "success": true,
  "epochId": "2025-10-01T16-00-00",
  "epochStart": "2025-10-01T16:00:00.000Z",
  "epochEnd": "2025-10-01T20:00:00.000Z",
  "nonce": "376ce532f42cf2a5",
  "targetListings": 10000,
  "tolerancePercent": 10,
  "submissionDeadline": "2025-10-01T20:00:00.000Z",
  "zipcodes": [
    {
      "zipcode": "19103",
      "expectedListings": 250,
      "state": "PA",
      "city": "Philadelphia",
      "county": "Philadelphia County",
      "marketTier": "PREMIUM",
      "geographicRegion": "Northeast"
    }
  ],
  "metadata": {
    "totalExpectedListings": 9420,
    "zipcodeCount": 25,
    "algorithmVersion": "v1.0",
    "selectionSeed": 12345
  }
}
```

### **2. Miner S3 Data Upload**
```http
POST /get-folder-access
```

**Purpose**: Get S3 upload credentials for miner data submission

**Request Body**:
```json
{
  "coldkey": "5F3sa2TJAWMqDhXG6jhV4N8ko9SxwGy8TpaNS1repo5EYjQX",
  "hotkey": "5H2WNbNfkRmHWJGdEUzZyVd7jZuP3BkwNDYgZQF8a1BcKwGx",
  "timestamp": 1696089600,
  "signature": "a1b2c3d4e5f6...",
  "expiry": 1696176000
}
```

### **3. Miner Status Updates**
```http
POST /api/v1/zipcode-assignments/status
```

**Purpose**: Update mining progress and completion status

**Request Body**:
```json
{
  "hotkey": "5H2WNbNfkRmHWJGdEUzZyVd7jZuP3BkwNDYgZQF8a1BcKwGx",
  "signature": "a1b2c3d4e5f6...",
  "timestamp": 1696089600,
  "epochId": "2025-10-01T16-00-00",
  "nonce": "376ce532f42cf2a5",
  "status": "COMPLETED",
  "listingsScraped": 1250,
  "zipcodesCompleted": [
    {
      "zipcode": "19103",
      "listingsScraped": 245,
      "completedAt": "2025-10-01T17:30:00.000Z"
    }
  ],
  "s3UploadComplete": true,
  "s3UploadTimestamp": "2025-10-01T17:45:00.000Z"
}
```

### **4. Validator S3 Access**
```http
POST /api/v1/s3-access/validator-upload
```

**Purpose**: Get S3 upload credentials for validator result storage

**Request Body**:
```json
{
  "hotkey": "5FHneW46xGXgs5mUiveU4sbTyGBzmstUspZC92UhjJM694ty",
  "signature": "a1b2c3d4e5f6...",
  "timestamp": 1696089600,
  "epochId": "2025-10-01T16-00-00",
  "purpose": "epoch_validation_results",
  "estimatedDataSizeMb": 25,
  "retentionDays": 90
}
```

### **5. System Statistics**
```http
GET /api/v1/zipcode-assignments/stats
```

**Purpose**: Get system-wide statistics for monitoring and decision-making

---

## üîÑ **Epoch System & Workflow**

### **Epoch Schedule**
- **Duration**: 4 hours per epoch
- **Schedule**: 00:00, 04:00, 08:00, 12:00, 16:00, 20:00 UTC
- **ID Format**: `YYYY-MM-DDTHH-00-00` (e.g., `2025-10-01T16-00-00`)

### **Epoch Lifecycle**
1. **PENDING** (15 minutes before start): Zipcode assignments generated
2. **ACTIVE** (4 hours): Miners can request assignments and submit data
3. **COMPLETED** (30 minutes): Validators process and score submissions
4. **ARCHIVED**: Results stored, next epoch begins

### **Zipcode Assignment Algorithm**
- **Target**: 10,000 total expected listings per epoch
- **Tolerance**: ¬±10% (9,000-11,000 listings)
- **Selection**: Weighted by market tier and historical performance
- **Anti-Gaming**: Honeypot zipcodes (5-10% of assignments)
- **Fairness**: Rotation algorithm prevents repeated assignments

---

## ü§ñ **Miner Integration Requirements**

### **1. Startup & Initialization**
```python
class ResiLabsMiner:
    def __init__(self, hotkey, coldkey, api_base_url):
        self.hotkey = hotkey
        self.coldkey = coldkey
        self.api = ResiLabsAPI(api_base_url, hotkey, coldkey)
        self.current_epoch = None
        self.assigned_zipcodes = []
        self.mining_status = "IDLE"
    
    def initialize(self):
        """Initialize miner and check API connectivity"""
        health = self.api.check_health()
        if not health['status'] == 'ok':
            raise Exception("API server not available")
        
        self.logger.info(f"Connected to API server v{health['version']}")
```

### **2. Epoch Detection & Assignment Request**
```python
def start_epoch_mining(self):
    """Request zipcode assignments for current epoch"""
    try:
        # Get current epoch assignments
        assignment_response = self.api.get_current_zipcodes()
        
        if not assignment_response['success']:
            self.logger.error("Failed to get zipcode assignments")
            return False
        
        self.current_epoch = {
            'id': assignment_response['epochId'],
            'start_time': assignment_response['epochStart'],
            'end_time': assignment_response['epochEnd'],
            'nonce': assignment_response['nonce'],
            'target_listings': assignment_response['targetListings'],
            'deadline': assignment_response['submissionDeadline']
        }
        
        self.assigned_zipcodes = assignment_response['zipcodes']
        self.mining_status = "IN_PROGRESS"
        
        self.logger.info(f"Epoch {self.current_epoch['id']}: Assigned {len(self.assigned_zipcodes)} zipcodes")
        
        # Start mining process
        return self.execute_mining_process()
        
    except Exception as e:
        self.logger.error(f"Epoch assignment failed: {e}")
        return False
```

### **3. Data Mining Process**
```python
def execute_mining_process(self):
    """Execute mining for assigned zipcodes"""
    completed_zipcodes = []
    total_listings = 0
    
    for zipcode_assignment in self.assigned_zipcodes:
        zipcode = zipcode_assignment['zipcode']
        expected_listings = zipcode_assignment['expectedListings']
        
        self.logger.info(f"Mining zipcode {zipcode} (target: {expected_listings} listings)")
        
        try:
            # Scrape real estate data for this zipcode
            listings_data = self.scrape_zipcode_data(zipcode, expected_listings)
            
            # Validate and format data
            formatted_data = self.format_listings_data(listings_data, zipcode)
            
            # Store locally for batch upload
            self.store_zipcode_data(zipcode, formatted_data)
            
            completed_zipcodes.append({
                'zipcode': zipcode,
                'listingsScraped': len(listings_data),
                'completedAt': datetime.utcnow().isoformat() + 'Z'
            })
            
            total_listings += len(listings_data)
            
            # Update progress periodically
            if len(completed_zipcodes) % 5 == 0:
                self.update_mining_status(completed_zipcodes, total_listings, "IN_PROGRESS")
                
        except Exception as e:
            self.logger.error(f"Failed to mine zipcode {zipcode}: {e}")
            continue
    
    # Upload all data to S3
    s3_success = self.upload_data_to_s3()
    
    # Final status update
    final_status = "COMPLETED" if s3_success else "FAILED"
    self.update_mining_status(completed_zipcodes, total_listings, final_status, s3_success)
    
    return s3_success
```

### **4. S3 Data Upload**
```python
def upload_data_to_s3(self):
    """Upload mined data to S3 using API credentials"""
    try:
        # Get S3 upload credentials
        s3_creds = self.api.get_s3_credentials()
        
        if not s3_creds['success']:
            self.logger.error("Failed to get S3 credentials")
            return False
        
        # Prepare data files
        data_files = self.prepare_data_files()
        
        # Upload each file to S3
        for file_path, file_data in data_files.items():
            upload_success = self.upload_file_to_s3(
                file_data, 
                s3_creds['uploadUrl'], 
                s3_creds['fields'],
                file_path
            )
            
            if not upload_success:
                self.logger.error(f"Failed to upload {file_path}")
                return False
        
        self.logger.info(f"Successfully uploaded {len(data_files)} files to S3")
        return True
        
    except Exception as e:
        self.logger.error(f"S3 upload failed: {e}")
        return False

def prepare_data_files(self):
    """Prepare data files in required format"""
    files = {}
    
    # Main listings data (Parquet format preferred)
    listings_df = self.compile_all_listings_data()
    files['listings.parquet'] = listings_df.to_parquet()
    
    # Metadata file
    metadata = {
        'epoch_id': self.current_epoch['id'],
        'miner_hotkey': str(self.hotkey.ss58_address),
        'mining_start_time': self.mining_start_time,
        'mining_end_time': datetime.utcnow().isoformat(),
        'total_listings': len(listings_df),
        'zipcodes_completed': len(self.assigned_zipcodes),
        'data_format_version': '2.0'
    }
    files['metadata.json'] = json.dumps(metadata, indent=2)
    
    return files
```

### **5. Status Updates**
```python
def update_mining_status(self, completed_zipcodes, total_listings, status, s3_complete=False):
    """Update mining status via API"""
    try:
        response = self.api.update_miner_status(
            epoch_id=self.current_epoch['id'],
            nonce=self.current_epoch['nonce'],
            status=status,
            listings_scraped=total_listings,
            zipcodes_completed=completed_zipcodes,
            s3_upload_complete=s3_complete,
            s3_upload_timestamp=datetime.utcnow().isoformat() + 'Z' if s3_complete else None
        )
        
        if response['success']:
            self.logger.info(f"Status updated: {status} ({total_listings} listings)")
        else:
            self.logger.error(f"Status update failed: {response.get('message', 'Unknown error')}")
            
    except Exception as e:
        self.logger.error(f"Status update error: {e}")
```

---

## üèÜ **Validator Integration Requirements**

### **1. Validator Initialization**
```python
class ResiLabsValidator:
    def __init__(self, hotkey, api_base_url):
        self.hotkey = hotkey
        self.api = ResiLabsAPI(api_base_url, hotkey)
        self.current_epoch = None
        self.miner_submissions = {}
        self.validation_results = {}
    
    def initialize(self):
        """Initialize validator and check API connectivity"""
        health = self.api.check_health()
        if not health['status'] == 'ok':
            raise Exception("API server not available")
        
        # Verify validator status
        if not self.api.verify_validator_status():
            raise Exception("Hotkey is not registered as validator")
        
        self.logger.info(f"Validator initialized for hotkey {self.hotkey.ss58_address}")
```

### **2. Epoch Monitoring & Data Collection**
```python
def monitor_epoch_completion(self):
    """Monitor for epoch completion and collect miner data"""
    while True:
        try:
            # Check system statistics
            stats = self.api.get_system_statistics()
            current_epoch = stats['epochStatus']['currentEpoch']
            
            # If epoch just completed, start validation process
            if (current_epoch['status'] == 'COMPLETED' and 
                current_epoch['id'] != self.last_processed_epoch):
                
                self.logger.info(f"Epoch {current_epoch['id']} completed, starting validation")
                self.validate_epoch(current_epoch)
                self.last_processed_epoch = current_epoch['id']
            
            time.sleep(60)  # Check every minute
            
        except Exception as e:
            self.logger.error(f"Epoch monitoring error: {e}")
            time.sleep(300)  # Wait 5 minutes on error
```

### **3. Miner Data Download & Analysis**
```python
def validate_epoch(self, epoch_info):
    """Download and validate all miner submissions for an epoch"""
    try:
        # Get list of miner submissions for this epoch
        submissions = self.api.get_epoch_submissions(epoch_info['id'])
        
        self.logger.info(f"Found {len(submissions)} miner submissions for epoch {epoch_info['id']}")
        
        validation_results = {}
        
        for submission in submissions:
            miner_hotkey = submission['minerHotkey']
            
            try:
                # Download miner's S3 data
                miner_data = self.download_miner_data(miner_hotkey, epoch_info['id'])
                
                # Validate data quality and accuracy
                validation_score = self.validate_miner_data(miner_data, submission)
                
                validation_results[miner_hotkey] = {
                    'score': validation_score,
                    'data_quality': validation_score['data_quality'],
                    'accuracy': validation_score['accuracy'],
                    'completeness': validation_score['completeness'],
                    'timeliness': validation_score['timeliness'],
                    'total_score': validation_score['total_score']
                }
                
                self.logger.info(f"Validated {miner_hotkey}: Score {validation_score['total_score']:.2f}")
                
            except Exception as e:
                self.logger.error(f"Failed to validate {miner_hotkey}: {e}")
                validation_results[miner_hotkey] = {
                    'score': 0.0,
                    'error': str(e)
                }
        
        # Upload validation results
        self.upload_validation_results(epoch_info['id'], validation_results)
        
        # Update Bittensor weights based on scores
        self.update_bittensor_weights(validation_results)
        
    except Exception as e:
        self.logger.error(f"Epoch validation failed: {e}")

def validate_miner_data(self, miner_data, submission_info):
    """Validate miner data quality and accuracy"""
    
    # Load miner's data files
    listings_df = pd.read_parquet(miner_data['listings.parquet'])
    metadata = json.loads(miner_data['metadata.json'])
    
    # Initialize scoring components
    scores = {
        'data_quality': 0.0,
        'accuracy': 0.0,
        'completeness': 0.0,
        'timeliness': 0.0
    }
    
    # 1. Data Quality Assessment (25% of score)
    quality_score = self.assess_data_quality(listings_df)
    scores['data_quality'] = quality_score
    
    # 2. Accuracy Verification (35% of score)
    # Sample and verify a subset of listings
    accuracy_score = self.verify_listing_accuracy(listings_df)
    scores['accuracy'] = accuracy_score
    
    # 3. Completeness Check (25% of score)
    # Check if all assigned zipcodes were attempted
    completeness_score = self.check_completeness(listings_df, submission_info)
    scores['completeness'] = completeness_score
    
    # 4. Timeliness Score (15% of score)
    # Check submission timing
    timeliness_score = self.assess_timeliness(metadata, submission_info)
    scores['timeliness'] = timeliness_score
    
    # Calculate weighted total score
    total_score = (
        scores['data_quality'] * 0.25 +
        scores['accuracy'] * 0.35 +
        scores['completeness'] * 0.25 +
        scores['timeliness'] * 0.15
    )
    
    scores['total_score'] = total_score
    
    return scores

def assess_data_quality(self, listings_df):
    """Assess data quality metrics"""
    quality_metrics = {
        'required_fields_present': 0.0,
        'data_types_correct': 0.0,
        'no_duplicates': 0.0,
        'reasonable_values': 0.0
    }
    
    # Check required fields
    required_fields = ['address', 'price', 'bedrooms', 'bathrooms', 'sqft', 'listing_date', 'zipcode']
    fields_present = sum(1 for field in required_fields if field in listings_df.columns)
    quality_metrics['required_fields_present'] = fields_present / len(required_fields)
    
    # Check data types
    type_score = self.validate_data_types(listings_df)
    quality_metrics['data_types_correct'] = type_score
    
    # Check for duplicates
    duplicate_rate = listings_df.duplicated().sum() / len(listings_df)
    quality_metrics['no_duplicates'] = max(0, 1.0 - duplicate_rate)
    
    # Check for reasonable values
    reasonable_score = self.validate_reasonable_values(listings_df)
    quality_metrics['reasonable_values'] = reasonable_score
    
    # Average all quality metrics
    return sum(quality_metrics.values()) / len(quality_metrics)

def verify_listing_accuracy(self, listings_df):
    """Verify accuracy by sampling and cross-checking listings"""
    
    # Sample 10% of listings for verification (minimum 5, maximum 50)
    sample_size = max(5, min(50, int(len(listings_df) * 0.1)))
    sample_listings = listings_df.sample(n=sample_size)
    
    verified_count = 0
    
    for _, listing in sample_listings.iterrows():
        try:
            # Attempt to verify listing exists and details are accurate
            is_accurate = self.verify_single_listing(listing)
            if is_accurate:
                verified_count += 1
        except Exception as e:
            self.logger.warning(f"Could not verify listing: {e}")
            continue
    
    accuracy_rate = verified_count / sample_size
    return accuracy_rate

def check_completeness(self, listings_df, submission_info):
    """Check if miner completed all assigned zipcodes"""
    
    assigned_zipcodes = set(submission_info['assignedZipcodes'])
    completed_zipcodes = set(listings_df['zipcode'].unique())
    
    # Calculate completion rate
    completion_rate = len(completed_zipcodes.intersection(assigned_zipcodes)) / len(assigned_zipcodes)
    
    # Check if target listing counts were met
    target_met_score = 0.0
    for zipcode in assigned_zipcodes:
        zipcode_listings = listings_df[listings_df['zipcode'] == zipcode]
        expected_count = submission_info['zipcodeTargets'].get(zipcode, 0)
        actual_count = len(zipcode_listings)
        
        # Score based on how close to target (within tolerance)
        if expected_count > 0:
            ratio = actual_count / expected_count
            if 0.9 <= ratio <= 1.1:  # Within 10% tolerance
                target_met_score += 1.0
            elif 0.8 <= ratio <= 1.2:  # Within 20% tolerance
                target_met_score += 0.7
            elif 0.7 <= ratio <= 1.3:  # Within 30% tolerance
                target_met_score += 0.4
    
    if len(assigned_zipcodes) > 0:
        target_met_score /= len(assigned_zipcodes)
    
    # Combine completion rate and target achievement
    return (completion_rate * 0.6) + (target_met_score * 0.4)
```

### **4. Validator Result Upload**
```python
def upload_validation_results(self, epoch_id, validation_results):
    """Upload validation results to S3"""
    try:
        # Get S3 upload credentials for validators
        s3_creds = self.api.get_validator_s3_credentials(epoch_id)
        
        if not s3_creds['success']:
            self.logger.error("Failed to get validator S3 credentials")
            return False
        
        # Prepare validation results file
        results_data = {
            'epoch_id': epoch_id,
            'validator_hotkey': str(self.hotkey.ss58_address),
            'validation_timestamp': datetime.utcnow().isoformat(),
            'validation_results': validation_results,
            'summary': {
                'total_miners': len(validation_results),
                'average_score': sum(r.get('total_score', 0) for r in validation_results.values()) / len(validation_results),
                'top_performers': sorted(validation_results.items(), key=lambda x: x[1].get('total_score', 0), reverse=True)[:10]
            }
        }
        
        # Upload to S3
        file_content = json.dumps(results_data, indent=2)
        upload_success = self.upload_file_to_s3(
            file_content,
            s3_creds['uploadUrl'],
            s3_creds['fields'],
            f"validation_results_{epoch_id}.json"
        )
        
        if upload_success:
            self.logger.info(f"Uploaded validation results for epoch {epoch_id}")
        
        return upload_success
        
    except Exception as e:
        self.logger.error(f"Failed to upload validation results: {e}")
        return False
```

### **5. Bittensor Weight Updates**
```python
def update_bittensor_weights(self, validation_results):
    """Update Bittensor weights based on validation scores"""
    try:
        # Convert validation scores to weight distribution
        weights = {}
        
        for miner_hotkey, results in validation_results.items():
            score = results.get('total_score', 0.0)
            
            # Convert score to weight (0.0 to 1.0)
            weight = max(0.0, min(1.0, score))
            weights[miner_hotkey] = weight
        
        # Normalize weights
        total_weight = sum(weights.values())
        if total_weight > 0:
            for hotkey in weights:
                weights[hotkey] = weights[hotkey] / total_weight
        
        # Update Bittensor weights
        success = self.subtensor.set_weights(
            wallet=self.wallet,
            netuid=46,
            uids=list(weights.keys()),
            weights=list(weights.values()),
            wait_for_inclusion=True
        )
        
        if success:
            self.logger.info(f"Updated Bittensor weights for {len(weights)} miners")
        
        return success
        
    except Exception as e:
        self.logger.error(f"Failed to update Bittensor weights: {e}")
        return False
```

---

## üìä **Data Formats & Standards**

### **Miner Data Upload Format**
```json
{
  "listings.parquet": "Binary Parquet file with listing data",
  "metadata.json": {
    "epoch_id": "2025-10-01T16-00-00",
    "miner_hotkey": "5H2WNbNfkRmHWJGdEUzZyVd7jZuP3BkwNDYgZQF8a1BcKwGx",
    "mining_start_time": "2025-10-01T16:05:00.000Z",
    "mining_end_time": "2025-10-01T19:45:00.000Z",
    "total_listings": 1250,
    "zipcodes_completed": 25,
    "data_format_version": "2.0"
  }
}
```

### **Required Listing Data Schema**
```python
REQUIRED_LISTING_FIELDS = {
    'address': str,           # Full property address
    'zipcode': str,          # 5-digit zipcode
    'price': float,          # Listing price in USD
    'bedrooms': int,         # Number of bedrooms
    'bathrooms': float,      # Number of bathrooms
    'sqft': int,            # Square footage
    'listing_date': str,     # ISO 8601 date when listed
    'property_type': str,    # 'house', 'condo', 'townhouse', etc.
    'listing_status': str,   # 'active', 'pending', 'sold'
    'days_on_market': int,   # Days since first listed
    'mls_id': str,          # MLS listing ID (if available)
    'source_url': str,       # URL where data was scraped
    'scraped_timestamp': str # ISO 8601 timestamp when scraped
}
```

### **Validator Results Format**
```json
{
  "epoch_id": "2025-10-01T16-00-00",
  "validator_hotkey": "5FHneW46xGXgs5mUiveU4sbTyGBzmstUspZC92UhjJM694ty",
  "validation_timestamp": "2025-10-01T20:30:00.000Z",
  "validation_results": {
    "miner_hotkey_1": {
      "score": {
        "data_quality": 0.85,
        "accuracy": 0.92,
        "completeness": 0.78,
        "timeliness": 0.95,
        "total_score": 0.87
      }
    }
  },
  "summary": {
    "total_miners": 150,
    "average_score": 0.73,
    "top_performers": [...]
  }
}
```

---

## üîí **Security & Anti-Gaming Measures**

### **1. Honeypot Detection**
- **Implementation**: 5-10% of zipcode assignments are honeypots
- **Detection**: Validators check for impossible/fake listings in honeypot zipcodes
- **Penalty**: Miners submitting honeypot data receive zero score

### **2. Signature Verification**
- **All API calls** require valid Bittensor hotkey signatures
- **Timestamp validation** prevents replay attacks (¬±5 minute window)
- **Nonce verification** ensures epoch authenticity

### **3. Rate Limiting**
- **Miner zipcode requests**: 10/minute per hotkey
- **S3 access requests**: 20/day per miner
- **Status updates**: 5/minute per hotkey

### **4. Data Validation**
- **Cross-verification**: Validators sample and verify listing accuracy
- **Duplicate detection**: Penalize duplicate listings across miners
- **Reasonable value checks**: Flag unrealistic property data

---

## ‚ö° **Performance & Scalability**

### **Expected Load**
- **Miners**: 100-500 active miners per epoch
- **Validators**: 10-50 active validators
- **API Requests**: ~10,000 requests per hour during peak
- **Data Volume**: 50-200GB per epoch (all miners combined)

### **Rate Limits**
- **Zipcode Assignment**: 10 requests/minute per hotkey
- **S3 Access**: 20 requests/day per miner
- **Status Updates**: 5 requests/minute per hotkey
- **System Stats**: 30 requests/minute (public)

### **Caching Strategy**
- **Zipcode assignments**: Cached for full epoch duration
- **System statistics**: Cached for 1 minute
- **S3 credentials**: Generated fresh, expire in 1 hour

---

## üö® **Error Handling & Monitoring**

### **Common Error Scenarios**
1. **API Server Unavailable**: Implement retry logic with exponential backoff
2. **Invalid Signatures**: Check timestamp and commitment format
3. **Rate Limit Exceeded**: Implement request queuing
4. **S3 Upload Failures**: Retry with different credentials
5. **Epoch Timing Issues**: Handle epoch transitions gracefully

### **Monitoring Requirements**
```python
# Health Check Integration
def monitor_api_health():
    """Continuously monitor API server health"""
    while True:
        try:
            health = requests.get(f"{API_BASE_URL}/healthcheck", timeout=10)
            if health.status_code != 200:
                logger.error(f"API health check failed: {health.status_code}")
                # Implement alerting logic
        except Exception as e:
            logger.error(f"API health check error: {e}")
        
        time.sleep(60)  # Check every minute

# Performance Metrics
METRICS_TO_TRACK = {
    'api_response_times': [],
    'successful_requests': 0,
    'failed_requests': 0,
    'data_upload_success_rate': 0.0,
    'average_mining_time': 0.0,
    'validation_scores': []
}
```

---

## üìã **Integration Checklist**

### **For Miner Developers**
- [ ] Implement API client with signature authentication
- [ ] Add epoch detection and zipcode assignment request
- [ ] Modify scraping logic to target assigned zipcodes only
- [ ] Implement S3 data upload with proper formatting
- [ ] Add status update reporting throughout mining process
- [ ] Implement error handling and retry logic
- [ ] Add health monitoring and alerting
- [ ] Test with development API server
- [ ] Validate data format compliance

### **For Validator Developers**
- [ ] Implement API client for validator endpoints
- [ ] Add epoch completion monitoring
- [ ] Implement miner data download from S3
- [ ] Create data validation and scoring algorithms
- [ ] Add validator result upload to S3
- [ ] Integrate with Bittensor weight setting
- [ ] Implement honeypot detection logic
- [ ] Add comprehensive logging and monitoring
- [ ] Test validation accuracy and performance

### **For Both**
- [ ] Update configuration for production API endpoints
- [ ] Implement proper logging and error reporting
- [ ] Add performance monitoring and metrics
- [ ] Create deployment and update procedures
- [ ] Document integration and troubleshooting guides

---

## ‚ùì **Questions for Development Team**

### **Technical Implementation Questions**

1. **Bittensor Integration**: What is the current structure of the miner and validator codebases? Are they using the standard Bittensor template or custom implementations?

**A: COMPREHENSIVE ANALYSIS COMPLETED**

**Current Architecture:**
- **Validator**: Forked from SN13 Data Universe with custom real estate validation logic
- **Miner**: Custom implementation with modular scraping architecture
- **Structure**: Both use standard Bittensor patterns but with significant customizations

**Key Components Identified:**
- **Miner Class** (`neurons/miner.py`): Main miner with S3PartitionedUploader integration
- **Validator Class** (`neurons/validator.py`): Main validator with MinerEvaluator and API monitoring
- **MinerEvaluator** (`vali_utils/miner_evaluator.py`): Synchronized batch evaluation system (100 miners per 4-hour cycle)
- **MinerScorer** (`rewards/miner_scorer.py`): Credibility-based scoring with exponential multiplier (credibility^2.5)
- **S3PartitionedUploader** (`upload_utils/s3_uploader.py`): Job-based S3 uploads with delta tracking

**Current Validation Process:**
1. Validators ping miners for property data requests
2. Cross-reference with Zillow using Szill scraper for accuracy verification
3. Check S3 uploads for duplicates and perform spot checks
4. Update miner credibility based on validation results
5. Set Bittensor weights based on final scores

**Proposed New System Alignment:**
‚úÖ **Perfect Match**: The proposed zipcode-based competitive system aligns with your reward distribution:
- Top 3 miners per zipcode: 55%, 30%, 10% rewards
- Remaining miners: 5% distributed
- Synthetic data miners: 0% rewards
- Only evaluate top 3 per zipcode (vs all 250 miners)

2. **Signature Library**: Which cryptographic library is currently being used for Bittensor signature generation?

**A: BITTENSOR PYTHON LIBRARY CONFIRMED**

**Current Implementation:**
```python
# From upload_utils/s3_utils.py and vali_utils/validator_s3_access.py
signature = wallet.hotkey.sign(commitment.encode())
signature_hex = signature.hex()
```

**Commitment Formats Currently Used:**
- **Miner S3 Access**: `"s3:data:access:{coldkey}:{hotkey}:{timestamp}"`
- **Validator S3 Access**: `"s3:validator:access:{timestamp}"`

**Library Details:**
- **Primary**: `bittensor` Python library (`wallet.hotkey.sign()`)
- **No Keyring**: No evidence of `@polkadot/keyring` usage found
- **Verification**: Uses standard Bittensor signature verification patterns

3. **Data Storage**: How is scraped data currently stored locally before submission? Should we maintain compatibility with existing storage formats?

**A: MAINTAIN EXISTING STORAGE - ALREADY SUPPORTS ZIPCODE UPLOADS**

**Current Local Storage:**
- **Database**: SQLite with configurable size limit (default 250GB)
- **Location**: `SqliteMinerStorage.sqlite` (configurable via `--neuron.database_name`)
- **Schema**: Uses `MinerStorage` interface with `SqliteMinerStorage` implementation
- **Management**: Automatic cleanup and size management

**Current S3 Upload Structure:**
‚úÖ **ALREADY SUPPORTS ZIPCODE-BASED UPLOADS**
```
S3 Structure: hotkey={miner_hotkey}/job_id={job_id}/data_{timestamp}_{count}.parquet
```

**Key Findings:**
- **Job-Based Uploads**: Current system uses `job_id` which can represent zipcode-specific jobs
- **Delta Uploads**: System already tracks `last_offset` per job to upload only new data
- **Parquet Format**: Data stored as compressed Parquet files with metadata
- **State Tracking**: `upload_utils/state_file.json` tracks processing state per job

**S3 Upload Pattern Analysis:**
```python
# From S3PartitionedUploader._upload_data_chunk()
s3_path = f"job_id={job_id}/{filename}"  # Already supports zipcode jobs
filename = f"data_{timestamp}_{len(raw_df)}.parquet"
```

**Recommendation**: ‚úÖ **NO CHANGES NEEDED**
- Current system already supports zipcode-based job uploads
- Simply configure job_ids to represent zipcode assignments
- Existing delta upload system prevents duplicate data
- Maintain all current storage formats and interfaces

4. **Validation Logic**: What are the current criteria for scoring miners? Should we maintain existing scoring weights or implement the new 4-component system (quality, accuracy, completeness, timeliness)?

**A: HYBRID APPROACH - ENHANCE EXISTING SYSTEM**

**Current Scoring System:**
```python
# From MinerScorer and DataValueCalculator
Raw Score = data_source_weight √ó job_weight √ó time_scalar √ó scorable_bytes
Final Score = Raw Score √ó (credibility ^ 2.5) + S3_boost
```

**Current Components:**
- **Credibility**: 0-1 scale, exponential multiplier (^2.5), EMA-based updates
- **Scorable Bytes**: Unique data contribution (1/N credit for shared data)
- **Time Scalar**: Data freshness depreciation
- **Job Weight**: Geographic priority (1.0-4.0x multiplier)
- **S3 Boost**: Storage validation bonus

**Recommended Integration:**
‚úÖ **ENHANCE EXISTING SYSTEM WITH NEW COMPONENTS**

**Mapping New ‚Üí Existing:**
- **Quality** ‚Üí Enhance existing `credibility` system
- **Accuracy** ‚Üí Integrate with existing validation results
- **Completeness** ‚Üí New component based on zipcode target achievement
- **Timeliness** ‚Üí Enhance existing `time_scalar` with submission timing

**Implementation Strategy:**
1. **Keep existing credibility system** (proven and stable)
2. **Add completeness scoring** for zipcode target achievement
3. **Enhance timeliness** with submission deadline penalties
4. **Maintain S3 boost** for storage validation
5. **Implement per-zipcode ranking** for top 3 reward distribution

5. **Error Handling**: What is the current approach to handling network failures, API timeouts, and data corruption? Should we implement specific retry strategies?

**A: ROBUST ERROR HANDLING ALREADY EXISTS - ENHANCE FOR API INTEGRATION**

**Current Error Handling Patterns:**

**1. Retry Logic with Exponential Backoff:**
```python
# From common/utils.py - async_run_with_retry()
async def async_run_with_retry(func, max_retries=3, delay_seconds=1, single_try_timeout=30):
    for attempt in range(1, max_retries + 1):
        try:
            return await func()
        except Exception as e:
            if attempt == max_retries:
                raise e
            time.sleep(delay_seconds)  # Constant backoff
```

**2. Validator API Monitoring:**
```python
# From neurons/validator.py - _start_api_monitoring()
def monitor_api():
    consecutive_failures = 0
    max_failures = 3  # Auto-restart after 3 failures
    # Monitors API health every 30 minutes with auto-restart
```

**3. S3 Upload Error Handling:**
- **Timeout Management**: 30-second timeouts for S3 operations
- **Credential Refresh**: Automatic retry with new credentials on auth failures
- **State Persistence**: Upload state tracking prevents data loss on failures

**4. Validation Error Handling:**
```python
# From scraping/youtube/youtube_custom_scraper.py
except Exception as e:
    if "429" in str(e) or "timeout" in str(e).lower():
        attempt += 1
        await asyncio.sleep(2 ** attempt)  # Exponential backoff
```

**Recommendations for API Integration:**
‚úÖ **ENHANCE EXISTING PATTERNS**
1. **Add exponential backoff** to `async_run_with_retry()` (currently constant)
2. **Implement circuit breaker** for API server failures
3. **Add request queuing** for rate limit handling
4. **Enhance monitoring** with health check integration

### **Operational Questions**

6. **Deployment Strategy**: How are miner and validator updates currently deployed? Do we need to maintain backward compatibility during the transition?

**A: PM2-BASED DEPLOYMENT WITH GRADUAL MIGRATION STRATEGY**

**Current Deployment Methods:**

**1. PM2 Process Management:**
```bash
# From docs/miner.md and docs/validator.md
pm2 start python --name testnet-miner -- ./neurons/miner.py \
    --netuid 428 --subtensor.network test \
    --wallet.name your_wallet --wallet.hotkey your_hotkey

pm2 start python --name testnet-validator -- ./neurons/validator.py \
    --netuid 428 --subtensor.network test
```

**2. Environment-Based Configuration:**
- **Testnet/Mainnet**: Auto-detection via `netuid` (428 vs 46)
- **S3 Auth URLs**: Auto-configured based on network
- **Configuration Files**: `.env` files for environment-specific settings

**3. Bootstrap Scripts:**
```bash
# Automated testnet setup
python bootstrap_testnet_428.py
```

**Backward Compatibility Requirements:**
‚úÖ **GRADUAL MIGRATION APPROACH NEEDED**

**Phase 1: Parallel Operation**
- Run old and new systems simultaneously
- New API integration as optional feature (`--enable_api_integration`)
- Maintain existing S3 upload patterns

**Phase 2: Validator Migration**
- Validators upgrade first to support both old and new miner formats
- Implement dual scoring: legacy + new zipcode-based
- Gradual weight shifting toward new system

**Phase 3: Miner Migration**
- Miners upgrade to API integration
- Maintain S3 upload compatibility
- Legacy miners continue working with reduced rewards

**Phase 4: Full Cutover**
- Disable legacy validation paths
- Full zipcode-based competitive mining
- Remove backward compatibility code

7. **Configuration Management**: How are API endpoints, credentials, and other configuration parameters currently managed? Should we use environment variables, config files, or a different approach?

**A: HYBRID APPROACH - ENVIRONMENT VARIABLES + AUTO-CONFIGURATION**

**Current Configuration System:**

**1. Environment Variables (.env files):**
```bash
# From .env and .env.backup
NETUID=428                                    # Network selection
SUBTENSOR_NETWORK=test                        # test/finney
WALLET_NAME=your_testnet_wallet_name
WALLET_HOTKEY=your_testnet_hotkey_name
RAPIDAPI_KEY=b869b7feb4msh25a74b696857db1p19cfd0jsnbc9d2e2e820f
S3_BUCKET=4000-resilabs-prod-bittensor-sn46-datacollection
```

**2. Auto-Configuration Logic:**
```python
# From neurons/miner.py and neurons/validator.py
if self.config.netuid == 428:  # Testnet
    if self.config.s3_auth_url == "https://api.resilabs.ai":
        self.config.s3_auth_url = "https://api-staging.resilabs.ai"
else:  # Mainnet
    self.config.s3_auth_url = "https://api.resilabs.ai"
```

**3. Command-Line Configuration:**
```bash
--netuid 428 --subtensor.network test --wallet.name your_wallet
--neuron.database_name SqliteMinerStorage_testnet.sqlite
--miner_upload_state_file upload_utils/state_file_testnet.json
```

**4. API Key Management:**
```python
# From vali_utils/api/auth/auth.py
self.master_key = os.getenv('MASTER_KEY')
self.metrics_api_key = os.getenv('METRICS_API_KEY', str(uuid4()))
```

**Recommendation for API Integration:**
‚úÖ **ENHANCE EXISTING SYSTEM**
- **Environment Variables**: For API endpoints and credentials
- **Auto-Configuration**: Network-based endpoint selection
- **Command-Line Overrides**: For development and testing
- **Secure Credential Management**: Environment-based API keys

8. **Monitoring & Alerting**: What monitoring systems are currently in place? Should we integrate with existing monitoring or implement new health checks?

**A: COMPREHENSIVE MONITORING INFRASTRUCTURE EXISTS**

**Current Monitoring Systems:**

**1. Built-in Health Checks:**
```python
# From vali_utils/api/routes.py
@router.get("/monitoring/system-status")
async def system_health_check():
    return {"status": "healthy", "timestamp": dt.datetime.utcnow().isoformat()}
```

**2. API Monitoring:**
```python
# From neurons/validator.py - Auto-restart monitoring
def monitor_api():
    consecutive_failures = 0
    max_failures = 3
    # Checks every 30 minutes, auto-restarts on 3 failures
```

**3. Metrics Collection:**
```python
# From vali_utils/api/routes.py
metrics.ON_DEMAND_VALIDATOR_QUERY_DURATION.labels(hotkey=hotkey, status=status).observe(duration)
metrics.ON_DEMAND_VALIDATOR_QUERY_ATTEMPTS.observe(attempt_i + 1)
```

**4. Logging Infrastructure:**
- **Bittensor Logging**: Integrated debug/info/error logging
- **File-based Logs**: Configurable logging directories
- **Structured Logging**: JSON-compatible log formats

**5. Diagnostic Tools:**
```bash
# From tools/README.md
python tools/check_miner_storage.py --netuid 428    # Quick health checks
python tools/validate_miner_storage.py --netuid 428 # Comprehensive validation
```

**6. Optional Integrations:**
```bash
# From .env files
WANDB_API_KEY=your_wandb_key          # Weights & Biases integration
METRICS_API_KEY=your_custom_metrics_key
```

**Recommendation for API Integration:**
‚úÖ **INTEGRATE WITH EXISTING MONITORING**
- **Extend health checks** to include API server status
- **Add API-specific metrics** (response times, error rates)
- **Enhance alerting** for epoch transition failures
- **Maintain existing diagnostic tools** with API integration support

9. **Testing Strategy**: What testing environments are available? Do we need to create mock API endpoints for development and testing?

**A: COMPREHENSIVE TESTING INFRASTRUCTURE EXISTS**

**Current Testing Environments:**

**1. Testnet Environment (Subnet 428):**
- **Purpose**: Full integration testing without TAO costs
- **Features**: 5-minute S3 uploads, auto-configured endpoints
- **S3 Auth**: `https://api-staging.resilabs.ai`
- **Bootstrap**: `python bootstrap_testnet_428.py`

**2. Mock Infrastructure:**
```python
# From tests/integration/fixtures/mock_s3.py
class MockS3AuthServer:    # Mock S3 authentication
class MockS3Uploader:      # Mock S3 upload operations
class MockS3Validator:     # Mock S3 validation

# From tests/mocks/zillow_api_client.py
class MockZillowAPIClient: # Mock Zillow API responses
```

**3. Integration Test Suite:**
```python
# From tests/integration/
test_failure_scenarios.py     # Network failure testing
test_*.py                     # Various integration tests
```

**4. Multi-Miner Testing:**
```bash
# From research/multi-miner-testing/
verify_experiment_health.py  # Health check automation
S3_UPLOAD_SETUP_GUIDE.md    # Testing setup guide
```

**5. Diagnostic Tools:**
```bash
python tools/check_miner_storage.py     # Quick validation
python tools/validate_miner_storage.py  # Full system test
```

**Recommendation for API Integration:**
‚úÖ **EXTEND EXISTING MOCK INFRASTRUCTURE**
- **Create MockResiLabsAPI** class for development
- **Extend testnet environment** for API integration testing
- **Add API-specific test scenarios** (epoch transitions, rate limiting)
- **Maintain existing mock patterns** for consistency

10. **Performance Requirements**: What are the expected performance characteristics? How many miners/validators need to be supported simultaneously?

**A: CURRENT SYSTEM SUPPORTS TARGET SCALE**

**Current Performance Characteristics:**

**1. Miner Scale:**
- **Active Miners**: Currently supports 100-250 miners
- **Evaluation Cycle**: 100 miners per 4-hour synchronized batch
- **S3 Upload Frequency**: 2 hours (mainnet), 5 minutes (testnet)
- **Database Limit**: 250GB per miner (configurable)

**2. Validator Scale:**
- **Active Validators**: 10-50 validators supported
- **Evaluation Method**: Synchronized batch processing
- **API Monitoring**: 30-minute health check cycles
- **Concurrent Requests**: Handles multiple validator queries

**3. Current Performance Metrics:**
```python
# From vali_utils/miner_evaluator.py
# 7x faster evaluation (every ~10.4 hours vs ~68 hours)
# 94% API utilization (186k calls/month vs 27.9k)
blocks_per_cycle = 1200  # 4-hour evaluation cycles
```

**4. Rate Limiting (Current S3 System):**
- **Daily Limit**: 20 S3 auth requests per miner per day
- **Upload Frequency**: Configurable (5 min testnet, 2 hours mainnet)
- **Concurrent Uploads**: Multiple miners supported simultaneously

**Expected Performance for New API System:**
‚úÖ **MEETS REQUIREMENTS**
- **Miners**: 100-500 active miners per epoch (‚úÖ Current: 100-250)
- **Validators**: 10-50 active validators (‚úÖ Current: 10-50)
- **API Requests**: ~10,000 requests/hour peak (‚úÖ Current system handles this)
- **Data Volume**: 50-200GB per epoch (‚úÖ Current: Individual miners handle 250GB)

**Performance Optimizations Needed:**
1. **API Response Time**: Target <200ms (enhance current system)
2. **Concurrent Epoch Requests**: Handle 100+ simultaneous zipcode requests
3. **Database Scaling**: Optimize for epoch transition queries
4. **Caching Strategy**: Implement epoch-based caching

### **Business Logic Questions**

11. **Scoring Algorithm**: Should the new validation scoring system completely replace the existing scoring, or should it be a gradual transition with weighted combinations?

**A: GRADUAL TRANSITION WITH WEIGHTED COMBINATIONS**

**Current Scoring System (Proven & Stable):**
```python
Raw Score = data_source_weight √ó job_weight √ó time_scalar √ó scorable_bytes
Final Score = Raw Score √ó (credibility ^ 2.5) + S3_boost
```

**Recommended Transition Strategy:**

**Phase 1: Parallel Scoring (Months 1-2)**
```python
Legacy Score = existing_algorithm()
Zipcode Score = new_zipcode_algorithm()
Final Score = (0.7 √ó Legacy Score) + (0.3 √ó Zipcode Score)
```

**Phase 2: Weighted Transition (Months 3-4)**
```python
Final Score = (0.3 √ó Legacy Score) + (0.7 √ó Zipcode Score)
```

**Phase 3: Full Zipcode System (Month 5+)**
```python
Final Score = zipcode_competitive_algorithm()
# Per-zipcode ranking: Top 3 get 55%, 30%, 10% + 5% distributed
```

**Benefits of Gradual Transition:**
- **Risk Mitigation**: Maintain proven scoring during transition
- **Miner Adaptation**: Allow miners time to optimize for new system
- **Validator Confidence**: Build trust in new scoring mechanism
- **Data Collection**: Compare scoring systems for validation

12. **Honeypot Strategy**: What percentage of assignments should be honeypots? How should honeypot zipcodes be selected and validated?

**A: 5-10% HONEYPOT STRATEGY WITH SMART SELECTION**

**Recommended Honeypot Configuration:**

**1. Percentage Allocation:**
- **5-10% of total zipcodes** per epoch should be honeypots
- **Target**: ~1-3 honeypot zipcodes per epoch (out of 25 total)
- **Randomization**: Vary percentage (5-10%) to prevent gaming

**2. Honeypot Selection Criteria:**
```python
# Honeypot zipcode characteristics:
honeypot_criteria = {
    "market_tier": "PREMIUM",           # High-value areas (more tempting)
    "expected_listings": 200-500,       # Moderate size (not obviously fake)
    "geographic_spread": True,          # Distribute across regions
    "recent_activity": False,           # Avoid recently assigned zipcodes
    "data_availability": "LIMITED"      # Areas with known data scarcity
}
```

**3. Honeypot Validation Strategy:**
- **Impossible Listings**: Inject fake properties that don't exist
- **Temporal Inconsistencies**: Properties with impossible dates/prices
- **Geographic Impossibilities**: Properties outside zipcode boundaries
- **Cross-Reference Validation**: Check against known property databases

**4. Detection & Penalties:**
```python
honeypot_penalties = {
    "synthetic_data_detected": 0.0,     # Zero score for honeypot submissions
    "credibility_penalty": -0.2,        # Reduce long-term credibility
    "flagging_threshold": 2,            # Flag after 2 honeypot failures
    "ban_threshold": 5                  # Consider banning after 5 failures
}
```

**5. Honeypot Generation:**
- **API-Generated**: Server creates honeypot properties per epoch
- **Nonce-Protected**: Honeypot data tied to epoch nonce
- **Validator-Verified**: All validators check for honeypot submissions
- **Audit Trail**: Track honeypot detection for analysis

13. **Data Retention**: How long should miner data and validator results be retained in S3? Are there compliance or cost considerations?

**A: TIERED RETENTION STRATEGY BASED ON CURRENT PATTERNS**

**Current S3 Usage Analysis:**
- **Miner Data**: Continuous uploads to `s3_partitioned_storage/` by hotkey
- **Validator Access**: Existing S3 validation system in place
- **Storage Structure**: `hotkey={miner_hotkey}/job_id={job_id}/data_*.parquet`

**Recommended Retention Policy:**

**1. Miner Data Retention:**
- **Active Epoch Data**: 30 days (for validation and appeals)
- **Historical Epoch Data**: 90 days (for trend analysis and credibility tracking)
- **Archive Data**: 1 year (compressed, for compliance and research)
- **Purge Policy**: Delete after 1 year unless flagged for investigation

**2. Validator Results Retention:**
- **Validation Results**: 90 days (for audit and appeals)
- **Scoring Data**: 6 months (for credibility calculations)
- **Audit Trails**: 1 year (for compliance and dispute resolution)
- **Summary Statistics**: Permanent (aggregated, anonymized)

**3. Cost Optimization:**
```python
s3_storage_classes = {
    "active_epoch": "STANDARD",           # 0-30 days
    "historical": "STANDARD_IA",          # 30-90 days  
    "archive": "GLACIER",                 # 90 days - 1 year
    "long_term": "DEEP_ARCHIVE"           # 1+ years (if needed)
}
```

**4. Compliance Considerations:**
- **Data Privacy**: No PII in property data (addresses are public record)
- **Audit Requirements**: Maintain validation trails for disputes
- **Geographic Compliance**: US-based S3 regions for real estate data
- **Backup Strategy**: Cross-region replication for critical data

14. **Epoch Timing**: Is the 4-hour epoch duration acceptable, or should it be configurable? How should epoch transitions be handled if miners are mid-scrape?

**A: 4-HOUR EPOCHS OPTIMAL WITH GRACEFUL TRANSITIONS**

**Current System Analysis:**
- **Evaluation Cycles**: Already uses 4-hour synchronized batches (1200 blocks)
- **S3 Upload Frequency**: 2 hours (mainnet), 5 minutes (testnet)
- **Miner Evaluation**: 100 miners per 4-hour cycle

**4-Hour Epoch Justification:**
‚úÖ **OPTIMAL DURATION**
- **Scraping Time**: Sufficient for 10,000 listings across 25 zipcodes
- **Competition Balance**: Long enough for quality, short enough for competition
- **Validator Processing**: Aligns with current 4-hour evaluation cycles
- **Network Load**: Manageable API request distribution

**Epoch Transition Handling:**

**1. Graceful Transition Strategy:**
```python
epoch_transition = {
    "submission_deadline": "epoch_end - 30_minutes",  # 30-min buffer
    "grace_period": "15_minutes",                     # Late submission window
    "validation_window": "30_minutes",                # Validator processing time
    "next_epoch_start": "epoch_end + 30_minutes"     # Overlap prevention
}
```

**2. Mid-Scrape Handling:**
- **Early Warning**: 30-minute deadline warning to miners
- **Partial Submissions**: Accept incomplete zipcode data with penalties
- **Rollover Protection**: Prevent double-counting across epochs
- **State Management**: Clear miner state between epochs

**3. Configurable Parameters:**
```python
# Make epoch duration configurable for future adjustments
EPOCH_DURATION_HOURS = 4        # Default: 4 hours
SUBMISSION_BUFFER_MINUTES = 30   # Deadline buffer
GRACE_PERIOD_MINUTES = 15        # Late submission window
```

**4. Failure Recovery:**
- **Epoch Extension**: Automatic 15-minute extension if >50% miners fail
- **Partial Epochs**: Continue with available data if <50% participation
- **Emergency Rollback**: Manual epoch reset for critical failures
- **Monitoring**: Real-time epoch health monitoring

15. **Failure Recovery**: How should the system handle partial failures (e.g., some miners complete, others fail)? Should there be minimum participation thresholds?

**A: ROBUST FAILURE RECOVERY WITH PARTICIPATION THRESHOLDS**

**Current System Resilience:**
- **Synchronized Evaluation**: 100 miners per 4-hour batch with failure tolerance
- **Validator Redundancy**: Multiple validators provide scoring redundancy
- **S3 Upload Resilience**: State tracking prevents data loss on upload failures

**Recommended Failure Recovery Strategy:**

**1. Participation Thresholds:**
```python
participation_thresholds = {
    "minimum_miners_per_epoch": 10,      # Absolute minimum for valid epoch
    "minimum_participation_rate": 0.3,    # 30% of registered miners
    "per_zipcode_minimum": 3,            # At least 3 miners per zipcode
    "validator_minimum": 3               # At least 3 validators for consensus
}
```

**2. Partial Failure Handling:**
- **Per-Zipcode Evaluation**: Continue with zipcodes that have ‚â•3 miners
- **Proportional Rewards**: Distribute rewards only among participating miners
- **Failure Penalties**: Non-participating miners receive credibility penalties
- **Data Quality**: Maintain quality standards regardless of participation

**3. Epoch Continuation Logic:**
```python
def handle_epoch_failures(epoch_data):
    participating_miners = count_valid_submissions(epoch_data)
    
    if participating_miners < MINIMUM_MINERS_PER_EPOCH:
        return extend_epoch(15_minutes)  # Emergency extension
    
    if participation_rate < 0.3:
        return partial_epoch_processing()  # Continue with available data
    
    return normal_epoch_processing()
```

**4. Cascade Failure Prevention:**
- **Validator Redundancy**: Multiple validators score independently
- **API Failover**: Backup API servers for critical transitions
- **Database Resilience**: Transaction rollback for incomplete epochs
- **Emergency Procedures**: Manual intervention protocols for critical failures

### **Integration Timeline Questions**

16. **Rollout Strategy**: Should this be a hard cutover or gradual migration? Can old and new systems run in parallel?

**A: GRADUAL MIGRATION WITH PARALLEL OPERATION**

**Recommended 6-Month Rollout Strategy:**

**Phase 1: Infrastructure Setup (Month 1)**
- **API Server Deployment**: Production ResiLabs API with zipcode endpoints
- **Testnet Integration**: Full API integration testing on Subnet 428
- **Validator Preparation**: Update validators to support dual scoring
- **Monitoring Setup**: Enhanced monitoring for API integration

**Phase 2: Parallel Operation (Months 2-3)**
```python
# Dual system operation
legacy_enabled = True
api_integration_enabled = True
scoring_weight = {"legacy": 0.8, "zipcode": 0.2}  # Gradual transition
```

**Benefits of Parallel Operation:**
- **Risk Mitigation**: Fallback to legacy system if API fails
- **Performance Comparison**: Validate new system against proven baseline  
- **Miner Adaptation**: Allow miners to gradually optimize for new system
- **Validator Confidence**: Build trust through side-by-side comparison

**Phase 3: Weight Transition (Month 4)**
```python
scoring_weight = {"legacy": 0.3, "zipcode": 0.7}  # Shift toward new system
```

**Phase 4: Full Migration (Month 5)**
```python
legacy_enabled = False
api_integration_enabled = True
scoring_weight = {"zipcode": 1.0}  # Full zipcode-based scoring
```

**Phase 5: Optimization (Month 6)**
- **Performance Tuning**: Optimize API response times and caching
- **Feature Enhancement**: Add advanced anti-gaming measures
- **Legacy Cleanup**: Remove deprecated code and endpoints

**Parallel System Requirements:**
‚úÖ **FULLY COMPATIBLE WITH EXISTING ARCHITECTURE**
- **Miner Compatibility**: Existing miners continue working during transition
- **Validator Support**: Validators handle both legacy and API-based miners
- **S3 Integration**: Maintain existing S3 upload patterns
- **Configuration**: Feature flags for gradual enablement

17. **Testing Phase**: How long should the testing phase be? What success criteria need to be met before full deployment?

**A: 4-WEEK COMPREHENSIVE TESTING PHASE**

**Testing Timeline & Success Criteria:**

**Week 1: Infrastructure Testing**
```python
success_criteria = {
    "api_uptime": ">99.5%",
    "response_time": "<200ms average",
    "concurrent_miners": "100+ simultaneous requests",
    "epoch_transitions": "24 successful transitions (6 per day)"
}
```

**Week 2: Integration Testing**
```python
integration_tests = {
    "miner_api_integration": "10+ miners successfully request zipcodes",
    "s3_upload_compatibility": "Existing S3 patterns work unchanged", 
    "validator_dual_scoring": "Legacy + zipcode scoring in parallel",
    "error_handling": "Graceful degradation on API failures"
}
```

**Week 3: Load Testing**
```python
load_test_targets = {
    "peak_concurrent_miners": "250 miners requesting assignments",
    "api_requests_per_hour": "10,000+ during epoch transitions",
    "database_performance": "Sub-100ms query response times",
    "memory_usage": "Stable under sustained load"
}
```

**Week 4: End-to-End Validation**
```python
e2e_validation = {
    "complete_epoch_cycles": "28 full 4-hour epochs (1 week)",
    "scoring_accuracy": "Zipcode scores correlate with legacy scores",
    "honeypot_detection": "100% synthetic data detection rate",
    "validator_consensus": "‚â•90% validator agreement on rankings"
}
```

**Go/No-Go Criteria for Production:**
- ‚úÖ **Zero critical bugs** in 72-hour stability test
- ‚úÖ **API availability** >99.9% during testing period
- ‚úÖ **Validator consensus** on scoring methodology
- ‚úÖ **Miner adoption** >50% successful API integration
- ‚úÖ **Performance benchmarks** met under peak load

18. **Validator Coordination**: How will validators coordinate the transition? Do they all need to upgrade simultaneously?

**A: STAGED VALIDATOR UPGRADE WITH CONSENSUS MECHANISM**

**Validator Upgrade Strategy:**

**1. Gradual Validator Migration:**
```python
validator_upgrade_phases = {
    "early_adopters": "20% of validators (weeks 1-2)",
    "majority_adoption": "60% of validators (weeks 3-4)", 
    "full_migration": "100% of validators (weeks 5-6)",
    "legacy_support": "Maintain backward compatibility throughout"
}
```

**2. Consensus Requirements:**
- **Minimum Threshold**: 30% of validators must support new system before activation
- **Scoring Consensus**: Validators vote on scoring algorithm changes
- **Emergency Rollback**: 60% validator consensus can trigger rollback
- **Version Compatibility**: New validators support both legacy and zipcode miners

**3. Coordination Mechanisms:**

**On-Chain Coordination:**
```python
# Validators signal readiness via blockchain
validator_readiness = {
    "api_integration_ready": True,
    "dual_scoring_enabled": True,
    "version": "2.0.0",
    "last_updated": block_number
}
```

**Communication Channels:**
- **Discord/Telegram**: Real-time coordination during upgrades
- **GitHub Issues**: Technical discussion and bug reports
- **Documentation**: Upgrade guides and troubleshooting
- **Emergency Contacts**: Direct communication for critical issues

**4. Upgrade Safety Measures:**
- **Backward Compatibility**: New validators continue scoring legacy miners
- **Gradual Weight Shifting**: Slowly increase zipcode scoring weight
- **Rollback Procedures**: Quick reversion if consensus fails
- **Health Monitoring**: Real-time validator performance tracking

**5. No Simultaneous Upgrade Required:**
‚úÖ **STAGGERED UPGRADE SUPPORTED**
- **Mixed Validator Network**: Legacy and new validators coexist
- **Consensus Weighting**: Gradually shift toward new scoring as adoption increases
- **Miner Protection**: Miners continue earning rewards during transition
- **Validator Incentives**: Early adopters get enhanced scoring capabilities

19. **Miner Communication**: How will miners be notified of the upgrade requirements? Is there a communication channel or documentation system?

**A: MULTI-CHANNEL COMMUNICATION STRATEGY**

**Current Communication Infrastructure:**
- **GitHub Repository**: Primary documentation and code updates
- **Documentation System**: Comprehensive guides in `/docs` and `/dev-docs`
- **README Files**: Clear setup and upgrade instructions
- **Bootstrap Scripts**: Automated setup tools (`bootstrap_testnet_428.py`)

**Recommended Communication Plan:**

**1. Pre-Announcement (4 weeks before)**
```markdown
# Communication Channels:
- GitHub Release Notes with migration timeline
- Discord/Telegram announcements (if channels exist)
- Documentation updates with upgrade guides
- Email notifications to registered miners (if available)
```

**2. Documentation Updates:**
```bash
# New documentation files to create:
docs/api-integration-guide.md          # Step-by-step API integration
docs/migration-checklist.md           # Upgrade checklist for miners
dev-docs/api-troubleshooting.md       # Common issues and solutions
examples/api-integration-example.py   # Code examples for integration
```

**3. Upgrade Notification System:**
```python
# In-client notifications for miners
upgrade_notification = {
    "message": "API integration available - upgrade recommended",
    "version": "2.0.0",
    "deadline": "2025-12-01",
    "documentation": "https://github.com/resi-labs-ai/resi/docs/api-integration-guide.md",
    "severity": "RECOMMENDED"  # OPTIONAL -> RECOMMENDED -> REQUIRED
}
```

**4. Gradual Communication Escalation:**
- **Week 1**: Optional upgrade announcement
- **Week 2**: Recommended upgrade with benefits explanation
- **Week 3**: Required upgrade with deadline notice
- **Week 4**: Final warning with legacy system deprecation notice

**5. Support Channels:**
- **GitHub Issues**: Technical support and bug reports
- **Documentation**: Comprehensive troubleshooting guides
- **Community Forums**: Peer-to-peer support (Discord/Telegram)
- **Direct Support**: Emergency contact for critical issues

20. **Rollback Plan**: What is the rollback strategy if critical issues are discovered after deployment?

**A: COMPREHENSIVE ROLLBACK STRATEGY WITH MULTIPLE SAFETY NETS**

**Rollback Triggers:**
```python
rollback_criteria = {
    "api_availability": "<95% uptime for 2+ hours",
    "validator_consensus": "<70% validators operational", 
    "miner_participation": "<30% successful epoch participation",
    "critical_bugs": "Data corruption or reward calculation errors",
    "performance_degradation": ">500ms average API response time"
}
```

**Rollback Procedures:**

**1. Immediate Rollback (0-15 minutes)**
```python
# Emergency feature flags
emergency_rollback = {
    "disable_api_integration": True,
    "enable_legacy_scoring": True,
    "pause_epoch_transitions": True,
    "notify_validators": "EMERGENCY_ROLLBACK_INITIATED"
}
```

**2. Validator Coordination (15-30 minutes)**
```python
# Validator rollback consensus
rollback_consensus = {
    "required_votes": "60% of active validators",
    "voting_window": "15 minutes",
    "automatic_rollback": "If API unavailable for >30 minutes",
    "manual_override": "Emergency admin intervention"
}
```

**3. Data Integrity Protection:**
```python
rollback_data_protection = {
    "epoch_state_backup": "Snapshot before each epoch transition",
    "scoring_state_rollback": "Revert to last known good state",
    "s3_data_preservation": "Maintain all uploaded data during rollback",
    "audit_trail": "Log all rollback actions for analysis"
}
```

**4. Recovery Procedures:**
- **Legacy System Reactivation**: Automatic fallback to proven scoring system
- **Miner Notification**: Immediate notification of rollback status
- **Validator Synchronization**: Ensure all validators revert to legacy scoring
- **Data Consistency**: Verify no data loss or corruption during rollback

**5. Post-Rollback Analysis:**
```python
post_rollback_analysis = {
    "root_cause_investigation": "Identify and document failure causes",
    "fix_development": "Develop and test fixes in isolated environment", 
    "re_deployment_criteria": "Enhanced testing before next attempt",
    "communication": "Transparent post-mortem to community"
}
```

**6. Rollback Testing:**
- **Regular Drills**: Monthly rollback procedure testing
- **Automated Testing**: Rollback scenarios in CI/CD pipeline
- **Documentation**: Detailed rollback playbooks for operators
- **Monitoring**: Real-time rollback capability verification

**Recovery Timeline:**
- **0-5 minutes**: Automatic detection and emergency flags
- **5-15 minutes**: Validator consensus and coordination
- **15-30 minutes**: Full legacy system restoration
- **30-60 minutes**: System verification and miner notification
- **1-24 hours**: Root cause analysis and fix development

---

## üìû **Support & Resources**

### **API Documentation**
- **Swagger UI**: https://api.resilabs.com/docs
- **OpenAPI JSON**: https://api.resilabs.com/docs-json
- **Health Check**: https://api.resilabs.com/healthcheck

### **Development Resources**
- **GitHub Repository**: [Link to be provided]
- **Integration Examples**: Available in `/examples` directory
- **Test Scripts**: Available in `/test-scripts` directory

### **Contact Information**
- **API Support**: [Contact details]
- **Integration Support**: [Contact details]
- **Emergency Contact**: [Contact details]

---

*Document Version: 1.0*  
*Last Updated: October 1, 2025*  
*Status: Ready for Implementation* üöÄ